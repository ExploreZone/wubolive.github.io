{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://blog.wubolive.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-11-30T09:49:48.963Z","updated":"2018-11-30T09:49:48.950Z","comments":false,"path":"/404.html","permalink":"http://blog.wubolive.com//404.html","excerpt":"","text":""},{"title":"关于","date":"2018-11-30T09:49:49.464Z","updated":"2018-11-30T09:49:48.955Z","comments":false,"path":"about/index.html","permalink":"http://blog.wubolive.com/about/index.html","excerpt":"","text":"一个来自南方的北漂少年 · 阿波Linux | 云计算 | DevOps | 数据库 | Python | 大数据本人比较喜欢使用Markdown语法写文章，所以之前用Hexo驱动在github.com中搭建了一个博客http://blog.wubolive.com，但因为管理不灵活而将博客迁移到Leanote 新浪微博：@那繁华之处"},{"title":"书单","date":"2018-11-30T09:49:48.958Z","updated":"2018-11-30T09:49:48.958Z","comments":false,"path":"books/index.html","permalink":"http://blog.wubolive.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-11-30T09:49:49.489Z","updated":"2018-11-30T09:49:48.960Z","comments":false,"path":"categories/index.html","permalink":"http://blog.wubolive.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-11-30T09:49:48.963Z","updated":"2018-11-30T09:49:48.962Z","comments":true,"path":"links/index.html","permalink":"http://blog.wubolive.com/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-11-30T09:49:49.521Z","updated":"2018-11-30T09:49:48.965Z","comments":false,"path":"tags/index.html","permalink":"http://blog.wubolive.com/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-11-30T09:49:48.964Z","updated":"2018-11-30T09:49:48.964Z","comments":false,"path":"repository/index.html","permalink":"http://blog.wubolive.com/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"Kubernetes 排错之 Pod 异常","slug":"k8s-pod-error","date":"2018-12-18T11:26:22.000Z","updated":"2018-12-18T11:15:27.720Z","comments":true,"path":"2018/12/18/k8s-pod-error/","link":"","permalink":"http://blog.wubolive.com/2018/12/18/k8s-pod-error/","excerpt":"","text":"本章介绍 Pod 运行异常的排错方法。一般来说，无论 Pod 处于什么异常状态，都可以执行以下命令来查看 Pod 的状态123kubectl get pod &lt;pod-name&gt; -o yaml 查看 Pod 的配置是否正确kubectl describe pod &lt;pod-name&gt; 查看 Pod 的事件kubectl logs &lt;pod-name&gt; [-c &lt;container-name&gt;] 查看容器日志这些事件和日志通常都会有助于排查 Pod 发生的问题。Pod 一直处于 Pending 状态Pending 说明 Pod 还没有调度到某个 Node 上面。可以通过 kubectl describe pod &lt;pod-name&gt; 命令查看到当前 Pod 的事件，进而判断为什么没有调度。可能的原因包括资源不足，集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 等资源HostPort 已被占用，通常推荐使用 Service 对外开放服务端口Pod 一直处于 Waiting 或 ContainerCreating 状态首先还是通过 kubectl describe pod命令查看到当前 Pod 的事件。可能的原因包括镜像拉取失败，比如配置了错误的镜像Kubelet 无法访问镜像（国内环境访问 gcr.io 需要特殊处理）私有镜像的密钥配置错误镜像太大，拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项）CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如无法配置 Pod 网络无法分配 IP 地址容器无法启动，需要检查是否打包了正确的镜像或者是否配置了正确的容器参数Pod 处于 ImagePullBackOff 状态这通常是镜像名称配置错误或者私有镜像的密钥配置错误导致。这种情况可以使用 docker pull &lt;image&gt; 来验证镜像是否可以正常拉取。如果是私有镜像，需要首先创建一个 docker-registry 类型的 Secret1kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL然后在容器中引用这个 Secret123456spec: containers: - name: private-reg-container image: &lt;your-private-image&gt; imagePullSecrets: - name: my-secretPod 一直处于 CrashLoopBackOff 状态CrashLoopBackOff 状态说明容器曾经启动了，但又异常退出了。此时可以先查看一下容器的日志12kubectl logs &lt;pod-name&gt;kubectl logs --previous &lt;pod-name&gt;这里可以发现一些容器退出的原因，比如容器进程退出健康检查失败退出此时如果还未发现线索，还可以到容器内执行命令来进一步查看退出原因1kubectl exec cassandra -- cat /var/log/cassandra/system.log如果还是没有线索，那就需要 SSH 登录该 Pod 所在的 Node 上，查看 Kubelet 或者 Docker 的日志进一步排查了12# 查询 Nodekubectl get pod &lt;pod-name&gt; -o widePod 处于 Error 状态通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括依赖的 ConfigMap、Secret 或者 PV 等不存在请求的资源超过了管理员设置的限制，比如超过了 - - LimitRange 等违反集群的安全策略，比如违反了 PodSecurityPolicy 等容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定Pod 处于 Terminating 或 Unknown 状态从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。想要删除这些状态的 Pod 有三种方法：从集群中删除该 Node。使用公有云时，kube-controller-manager 会在 VM 删除后自动删除对应的 Node。而在物理机部署的集群中，需要管理员手动删除 Node（如 kubectl delete node &lt;node-name&gt;。Node 恢复正常。Kubelet 会重新跟 kube-apiserver 通信确认这些 Pod 的期待状态，进而再决定删除或者继续运行这些 Pod。用户强制删除。用户可以执行 kubectl delete pods &lt;pod&gt; --grace-period=0 --force 强制删除 Pod。除非明确知道 Pod 的确处于停止状态（比如 Node 所在 VM 或物理机已经关机），否则不建议使用该方法。特别是 StatefulSet 管理的 Pod，强制删除容易导致脑裂或者数据丢失等问题。Pod 行为异常这里所说的行为异常是指 Pod 没有按预期的行为执行，比如没有运行 podSpec 里面设置的命令行参数。这一般是 podSpec yaml 文件内容有误，可以尝试使用 –validate 参数重建容器，比如12kubectl delete pod mypodkubectl create --validate -f mypod.yaml也可以查看创建后的 podSpec 是否是对的，比如1kubectl get pod mypod -o yaml修改静态 Pod 的 Manifest 后未自动重建Kubelet 使用 inotify 机制检测 /etc/kubernetes/manifests 目录（可通过 Kubelet 的 --pod-manifest-path 选项指定）中静态 Pod 的变化，并在文件发生变化后重新创建相应的 Pod。但有时也会发生修改静态 Pod 的 Manifest 后未自动创建新 Pod 的情景，此时一个简单的修复方法是重启 Kubelet。","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/categories/K8s/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/tags/K8s/"}]},{"title":"高可用源码部署Kubernetes","slug":"k8s-install","date":"2018-12-18T08:17:28.000Z","updated":"2018-12-18T10:45:47.063Z","comments":true,"path":"2018/12/18/k8s-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/18/k8s-install/","excerpt":"","text":"角色IP组件镜像仓库hub.wubolive.comHarbork8s-master1192.168.0.11kube-apiserver,kube-controller-manage,kube-scheduler,etcd,keepalivedk8s-master2192.168.0.12kube-apiserver,kube-controller-manage,kube-scheduler,etcd,keepalivedk8s-node1192.168.0.13kubelet,kube-proxy,docker,flannel,etcdk8s-node2192.168.0.14kubelet,kube-proxy,docker,flannelKeeplived192.168.0.10由master1和master2配置的VIP地址简介Master组件kube-apiserver: Kubernetes API，集群的统一入口，各组件协调者，以RESTful API提供接口服务，所有对象资源的增删改查和监听操作都交给APIServer处理后再提交给Etcd存储。kube-controller-manager: 处理集群中常规后台任务，一个资源对应一个控制器，而ControllerManager就是负责管理这些控制器的。kube-scheduler: 根据调度算法为新创建的Pod选择一个Node节点，可以任意部署,可以部署在同一个节点上,也可以部署在不同的节点上Node组件kubelet: kubelet是Master在Node节点上的Agent，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。kubelet将每个Pod转换成一组容器。kube-proxy: 在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作。docker： 容器引擎etcd: 分布式键值存储系统。用于保存集群状态数据，比如Pod、Service等对象信息。部署Etcd集群准备工作使用cfssl来生成自签证书，先下载cfssl工具：1234567[root@k8s-master1 ~]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64[root@k8s-master1 ~]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64[root@k8s-master1 ~]# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64[root@k8s-master1 ~]# chmod +x cfssl*[root@k8s-master1 ~]# mv cfssl_linux-amd64 /usr/local/bin/cfssl[root@k8s-master1 ~]# mv cfssljson_linux-amd64 /usr/local/bin/cfssljson[root@k8s-master1 ~]# mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfss-certinfo生成Etcd证书编写CA配置文件:1234567891011121314151617181920[root@k8s-master1 ~]# touch ca-config.json ca-csr.json server-csr.json[root@k8s-master1 ~]# cat ca-config.json &#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;www&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;编写证书签名请求配置文件:123456789101112131415[root@k8s-master1 ~]# cat ca-csr.json &#123; &quot;CN&quot;: &quot;etcd CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125;编写etcd集群证书配置文件：123456789101112131415161718192021[root@k8s-master1 ~]# cat server-csr.json &#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;192.168.0.11&quot;, &quot;192.168.0.12&quot;, &quot;192.168.0.13&quot;, &quot;192.168.0.14&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot; &#125; ]&#125;生成证书和密钥12345[root@k8s-master1 ~]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -[root@k8s-master1 ~]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server[root@k8s-master1 ~]# ls *.pemca-key.pem ca.pem server-key.pem server.pem# 生成证书时注意输出信息,如果配置文件格式有问题就会报错安装Etcd二进制包下载地址：https://github.com/coreos/etcd/releases/tag/v3.2.12以下步骤在规划的四个节点操作是一样的，除了etcd配置文件中的服务器IP和ETCD_NAME。下载并解压二进制包123456[root@k8s-master1 ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.2.12/etcd-v3.2.12-linux-amd64.tar.gz[root@k8s-master1 ~]# mkdir -p /home/etcd/&#123;bin,conf,data,ssl&#125;[root@k8s-master1 ~]# tar zxf etcd-v3.2.12-linux-amd64.tar.gz[root@k8s-master1 ~]# mv etcd-v3.2.12-linux-amd64/&#123;etcd,etcdctl&#125; /home/etcd/bin/[root@k8s-master1 ~]# ln -s /home/etcd/bin/etcd /usr/local/bin/etcd[root@k8s-master1 ~]# ln -s /home/etcd/bin/etcdctl /usr/local/bin/etcdctl创建etcd配置文件 (注意ip地址及节点名称)。12345678910111213[root@k8s-master1 ~]# vim /home/etcd/conf/etcd.conf#[Member]ETCD_NAME=&quot;etcd01&quot;ETCD_DATA_DIR=&quot;/home/etcd/data&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.0.11:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.0.11:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.0.11:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.0.11:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://192.168.0.11:2380,etcd02=https://192.168.0.12:2380,etcd03=https://192.168.0.13:2380,etcd04=https://192.168.0.14:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;说明：ETCD_NAME 节点名称ETCD_DATA_DIR 数据目录ETCD_LISTEN_PEER_URLS 集群通信监听地址ETCD_LISTEN_CLIENT_URLS 客户端访问监听地址ETCD_INITIAL_ADVERTISE_PEER_URLS 集群通告地址ETCD_ADVERTISE_CLIENT_URLS 客户端通告地址ETCD_INITIAL_CLUSTER 集群节点地址ETCD_INITIAL_CLUSTER_TOKEN 集群TokenETCD_INITIAL_CLUSTER_STATE 加入集群的当前状态，new是新集群，existing表示加入已有集群systemctl管理etcd：12345678910111213141516171819202122232425262728293031[root@k8s-master1 ~]# vim /usr/lib/systemd/system/etcd.service [Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notify EnvironmentFile=/home/etcd/conf/etcd.confExecStart=/home/etcd/bin/etcd \\--name=$&#123;ETCD_NAME&#125; \\--data-dir=$&#123;ETCD_DATA_DIR&#125; \\--listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \\--listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \\--advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \\--initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \\--initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \\--initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \\--initial-cluster-state=new \\--cert-file=/home/etcd/ssl/server.pem \\--key-file=/home/etcd/ssl/server-key.pem \\--peer-cert-file=/home/etcd/ssl/server.pem \\--peer-key-file=/home/etcd/ssl/server-key.pem \\--trusted-ca-file=/home/etcd/ssl/ca.pem \\--peer-trusted-ca-file=/home/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target将生成的证书文件拷贝到配置文件中的位置。1[root@k8s-master1 ~]# cp ca*.pem server*.pem /home/etcd/ssl/启动Etcd集群在各节点中运行12[root@k8s-master1 ~]# systemctl start etcd[root@k8s-master1 ~]# systemctl enable etcd启动成功后，检查Etcd集群状态：123456[root@k8s-master1 ~]# etcdctl --ca-file=/home/etcd/ssl/ca.pem --cert-file=/home/etcd/ssl/server.pem --key-file=/home/etcd/ssl/server-key.pem --endpoints=&quot;https://192.168.0.11:2379,https://192.168.0.12:2379,https://192.168.0.13:2379,https://192.168.0.14:2379&quot; cluster-healthmember b8fffb7f5b2f26e is healthy: got healthy result from https://192.168.0.12:2379member cb18584c4f4dbfc is healthy: got healthy result from https://192.168.0.11:2379member 1678ac552db73f86 is healthy: got healthy result from https://192.168.0.14:2379member 32cf7f46292d6a20 is healthy: got healthy result from https://192.168.0.13:2379cluster is healthy上述信息表上部署成功，如果有问题可执行journalctl -u etcd查看报错信息。在Node节点安装Docker从国内源下载并安装docker-ce123[root@k8s-node1 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2[root@k8s-node1 ~]# yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo[root@k8s-node1 ~]# yum install docker-ce -y配置docker国内镜像源并启动123[root@k8s-node1 ~]# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://bc437cce.m.daocloud.io[root@k8s-node1 ~]# systemctl start docker[root@k8s-node1 ~]# systemctl enable docker部署Flannel网络向etcd 写入集群Pod 网段信息该步骤只需在第一次部署Flannel 网络时执行，后续在其他节点上部署Flanneld 时无需再写入该信息1234567[root@k8s-master1 ~]# cd /home/etcd/ssl/[root@k8s-master1 ssl]# /home/etcd/bin/etcdctl \\--ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem \\--endpoints=&quot;https://192.168.0.11:2379,https://192.168.0.12:2379,https://192.168.0.13:2379,https://192.168.0.14:2379&quot; \\set /coreos.com/network/config &apos;&#123; &quot;Network&quot;: &quot;172.30.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos;#返回信息：&#123; &quot;Network&quot;: &quot;172.30.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;安装和配置flanneld(node节点)下载二进制包:1234[root@k8s-node1 ~]# wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz[root@k8s-node1 ~]# tar zxf flannel-v0.10.0-linux-amd64.tar.gz [root@k8s-node1 ~]# mkdir /home/kubernetes/&#123;conf,bin&#125; -p[root@k8s-node1 ~]# mv flanneld mk-docker-opts.sh /home/kubernetes/bin/配置Flannel:12[root@k8s-node1 ~]# vim /home/kubernetes/conf/flanneld.confFLANNEL_OPTIONS=&quot;--etcd-endpoints=https://192.168.0.11:2379,https://192.168.0.12:2379,https://192.168.0.13:2379,https://192.168.0.14:2379 -etcd-cafile=/home/etcd/ssl/ca.pem -etcd-certfile=/home/etcd/ssl/server.pem -etcd-keyfile=/home/etcd/ssl/server-key.pem&quot;配置systemcatl管理flanneld123456789101112131415[root@k8s-node1 ~]# vim /usr/lib/systemd/system/flanneld.service[Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]Type=notifyEnvironmentFile=/home/kubernetes/conf/flanneld.confExecStart=/home/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONSExecStartPost=/home/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure[Install]WantedBy=multi-user.target更改Docker配置指定子网段123456789101112131415161718192021222324[root@k8s-node1 ~]# vim /usr/lib/systemd/system/docker.service [Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notifyEnvironmentFile=/run/flannel/subnet.envExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONSExecReload=/bin/kill -s HUP $MAINPIDLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.target重启flannel和docker1234[root@k8s-node1 ~]# systemctl daemon-reload[root@k8s-node1 ~]# systemctl start flanneld[root@k8s-node1 ~]# systemctl enable flanneld[root@k8s-node1 ~]# systemctl restart docker检查配置是否生效1234567891011121314151617181920212223# node1[root@k8s-node1 ~]# ps -ef | grep dockerroot 27850 1 0 02:25 ? 00:00:00 /usr/bin/dockerd --bip=172.30.35.1/24 --ip-masq=false --mtu=14504: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN link/ether 02:42:bc:2c:a1:96 brd ff:ff:ff:ff:ff:ff inet 172.30.35.1/24 brd 172.30.35.255 scope global docker0 valid_lft forever preferred_lft forever5: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN link/ether f6:a6:91:34:86:fe brd ff:ff:ff:ff:ff:ff inet 172.30.35.0/32 scope global flannel.1 valid_lft forever preferred_lft forever# node2[root@k8s-node2 ~]# ps -ef | grep dockerroot 27215 1 0 02:24 ? 00:00:00 /usr/bin/dockerd --bip=172.30.16.1/24 --ip-masq=false --mtu=14504: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN link/ether 02:42:de:a7:53:8f brd ff:ff:ff:ff:ff:ff inet 172.30.16.1/24 brd 172.30.16.255 scope global docker0 valid_lft forever preferred_lft forever5: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN link/ether 72:a3:0f:f4:a7:0e brd ff:ff:ff:ff:ff:ff inet 172.30.16.0/32 scope global flannel.1 valid_lft forever preferred_lft forever请确docker0与flannel.1在同一网段，然后测试两台主机的docker0通信12345# 从node1上ping node2的docker0IP[root@k8s-node1 ~]# ping 172.30.16.1PING 172.30.16.1 (172.30.16.1) 56(84) bytes of data.64 bytes from 172.30.16.1: icmp_seq=1 ttl=64 time=0.459 ms64 bytes from 172.30.16.1: icmp_seq=2 ttl=64 time=1.06 ms能ping通说明flannel部署成功在Master上部署组件在安装完上面etcd,flannel,docker后接下来开始安装master组件。生成证书创建CA证书123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master1 ~]# mkdir -p /home/kubernetes/&#123;ssl,conf,bin&#125;[root@k8s-master1 ~]# cd /home/kubernetes/ssl/[root@k8s-master1 ssl]# vim ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;[root@k8s-master1 ssl]# vim ca-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;[root@k8s-master1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -生成apiserver证书12345678910111213141516171819202122232425262728293031[root@k8s-master1 ssl]# vim server-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.0.10&quot;, &quot;192.168.0.11&quot;, &quot;192.168.0.12&quot;, &quot;192.168.0.13&quot;, &quot;192.168.0.14&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;[root@k8s-master1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server生成kube-proxy证书:12345678910111213141516171819[root@k8s-master1 ssl]# vim kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;[root@k8s-master1 ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy完成如上操作将会生成以下证书文件:12[root@k8s-master1 ssl]# ls *.pemca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem部署apiserver组件到gitlab上下载二进制包 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md1234[root@k8s-master1 ~]# wget https://dl.k8s.io/v1.11.5/kubernetes-server-linux-amd64.tar.gz[root@k8s-master1 ~]# tar zxf kubernetes-server-linux-amd64.tar.gz[root@k8s-master1 ~]# cd kubernetes/server/bin/[root@k8s-master1 bin]# cp kube-apiserver kube-scheduler kube-controller-manager kubectl /home/kubernetes/bin/生成Token并创建token文件1234[root@k8s-master1 ~]# head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;29cd86c2fd73035417e38c53bc8f12d9[root@k8s-master1 ~]# vim /home/kubernetes/conf/token.csv 29cd86c2fd73035417e38c53bc8f12d9,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;创建apiserver配置文件12345678910111213141516171819202122[root@k8s-master1 ~]# vim /home/kubernetes/conf/kube-apiserverKUBE_APISERVER_OPTS=&quot;--logtostderr=true \\--v=4 \\--etcd-servers=https://192.168.0.11:2379,https://192.168.0.12:2379,https://192.168.0.13:2379,https://192.168.0.14:2379 \\--bind-address=0.0.0.0 \\--insecure-bind-address=0.0.0.0 \\--secure-port=6443 \\--advertise-address=192.168.0.11 \\--allow-privileged=true \\--service-cluster-ip-range=192.168.0.0/24 \\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC,Node \\--enable-bootstrap-token-auth \\--token-auth-file=/home/kubernetes/conf/token.csv \\--service-node-port-range=30000-50000 \\--tls-cert-file=/home/kubernetes/ssl/server.pem \\--tls-private-key-file=/home/kubernetes/ssl/server-key.pem \\--client-ca-file=/home/kubernetes/ssl/ca.pem \\--service-account-key-file=/home/kubernetes/ssl/ca-key.pem \\--etcd-cafile=/home/etcd/ssl/ca.pem \\--etcd-certfile=/home/etcd/ssl/server.pem \\--etcd-keyfile=/home/etcd/ssl/server-key.pem&quot;参数说明：--logtostderr 启用日志---v 日志等级--etcd-servers etcd集群地址--bind-address 监听地址--secure-port https安全端口--advertise-address 集群通告地址--allow-privileged 启用授权--service-cluster-ip-range Service虚拟IP地址段--enable-admission-plugins 准入控制模块--authorization-mode 认证授权，启用RBAC授权和节点自管理--enable-bootstrap-token-auth 启用TLS bootstrap功能，后面会讲到--token-auth-file token文件--service-node-port-range Service Node类型默认分配端口范围使用systemctl管理apiserver123456789101112[root@k8s-master1 ~]# vim /usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/home/kubernetes/conf/kube-apiserverExecStart=/home/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target启动kube-apiserver12[root@k8s-master1 ~]# systemctl start kube-apiserver[root@k8s-master1 ~]# systemctl enable kube-apiserver部署schduler组件创建schduler配置文件12345[root@k8s-master1 ~]# vim /home/kubernetes/conf/kube-schedulerKUBE_SCHEDULER_OPTS=&quot;--logtostderr=true \\--v=4 \\--master=127.0.0.1:8080 \\--leader-elect&quot;参数说明:--master 连接本地apiserver--leader-elect 当该组件启动多个时，自动选举（HA）使用systemd管理schduler组件123456789101112[root@k8s-master1 ~]# vim /usr/lib/systemd/system/kube-scheduler.service [Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/home/kubernetes/conf/kube-schedulerExecStart=/home/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target启动kube-scheduler12[root@k8s-master1 ~]# systemctl enable kube-scheduler [root@k8s-master1 ~]# systemctl start kube-scheduler部署controller-manager组件创建controller-manager配置文件：123456789101112[root@k8s-master1 ~]# vim /home/kubernetes/conf/kube-controller-manager KUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=true \\--v=4 \\--master=127.0.0.1:8080 \\--leader-elect=true \\--address=127.0.0.1 \\--service-cluster-ip-range=192.168.0.0/24 \\--cluster-name=kubernetes \\--cluster-signing-cert-file=/home/kubernetes/ssl/ca.pem \\--cluster-signing-key-file=/home/kubernetes/ssl/ca-key.pem \\--root-ca-file=/home/kubernetes/ssl/ca.pem \\--service-account-private-key-file=/home/kubernetes/ssl/ca-key.pem&quot;使用systemctl管理controller-manager组件123456789101112[root@k8s-master1 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/home/kubernetes/conf/kube-controller-managerExecStart=/home/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target启动controller-manager12[root@k8s-master1 ~]# systemctl start kube-controller-manager[root@k8s-master1 ~]# systemctl enable kube-controller-manager检查当前集群状态12345678910111213141516# 配置kubectl命令环境变量[root@k8s-master1 ~]# vim /etc/profileK8S_HOME=/home/kubernetesETCD_HOME=/home/etcdexport PATH=$PATH:$K8S_HOME/bin:$ETCD_HOME/bin[root@k8s-master1 ~]# source /etc/profile# 查看组件状态[root@k8s-master1 ~]# kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-3 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;输出如上信息，说明所有组件都正常，然后在master2上也部署一遍配置kube-apiserver高可用按照上面的方式在master1与master2机器上安装kube-apiserver、kube-controller-manager、kube-scheduler，但是kube-apiserver还不是高可用的，如果master1 这个节点down 掉了，那么我们api就不能正常提供服务了。这里我们可以使用两种方法来实现高可用方式1：使用阿里云SLB这种方式实际上是最省心的，在阿里云上建一个内网的SLB，将master1 与master2 添加到SLB 机器组中，转发8080(http)和6443(https)端口即可方式2：使用keepalivedKeepAlived 是一个高可用方案，通过 VIP（即虚拟 IP）和心跳检测来实现高可用开启路由转发master1与master2 节点都需要安装keepalived,这里我们定义虚拟IP为：192.168.0.1012345678910[root@k8s-master1 ~]# vi /etc/sysctl.conf# 添加以下内容net.ipv4.ip_forward = 1net.ipv4.ip_nonlocal_bind = 1# 验证并生效[root@k8s-master1 ~]# sysctl -p# 验证是否生效[root@k8s-master1 ~]# cat /proc/sys/net/ipv4/ip_forward1安装keepalived:1[root@k8s-master1 ~]# yum install -y keepalived配置keepalived我们这里将master1 设置为Master，master2 设置为Backup，修改配置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475[root@k8s-master1 ~]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; &#125; router_id kube_api&#125;vrrp_script check_k8s-api &#123; # 自身状态检测 script &quot;killall -0 kube-apiserver&quot; interval 3 weight 5&#125;vrrp_instance k8s-api-vip &#123; # 使用单播通信，默认是组播通信 unicast_src_ip 192.168.0.11 unicast_peer &#123; 192.168.0.12 &#125; # 初始化状态 state MASTER # 虚拟ip 绑定的网卡 （这里根据你自己的实际情况选择网卡） interface eno16777736 # 此ID 要与Backup 配置一致 virtual_router_id 51 # 默认启动优先级，要比Backup 大点，但要控制量，保证自身状态检测生效 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; # 虚拟ip 地址 192.168.0.10 &#125; track_script &#123; check_k8s-api &#125;&#125;virtual_server 192.168.0.10 8080 &#123; delay_loop 5 lvs_sched wlc lvs_method NAT persistence_timeout 1800 protocol TCP real_server 192.168.0.11 8080 &#123; weight 1 TCP_CHECK &#123; connect_port 8080 connect_timeout 3 &#125; &#125;&#125;virtual_server 192.168.0.10 6443 &#123; delay_loop 5 lvs_sched wlc lvs_method NAT persistence_timeout 1800 protocol TCP real_server 192.168.0.11 6443 &#123; weight 1 TCP_CHECK &#123; connect_port 6443 connect_timeout 3 &#125; &#125;&#125;统一的方式在master2 节点上安装keepalived，修改配置，只需要将state 更改成BACKUP，priority更改成99，unicast_src_ip 与unicast_peer 地址修改即可。启动并验证keepalived1234567891011[root@k8s-master1 ~]# systemctl start keepalived[root@k8s-master1 ~]# systemctl enable keepalived[root@k8s-master1 ~]# ip addr3: eno33554960: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:9d:01:ba brd ff:ff:ff:ff:ff:ff inet 192.168.0.11/24 brd 192.168.0.255 scope global eno33554960 valid_lft forever preferred_lft forever inet 192.168.0.10/32 scope global eno33554960 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe9d:1ba/64 scope link valid_lft forever preferred_lft forever本地访问vip+端口进行访问测试1234567[root@k8s-master1 ~]# curl -I 192.168.0.10:80HTTP/1.1 200 OKContent-Type: application/jsonDate: Mon, 10 Dec 2018 17:21:49 GMT[root@k8s-master1 ~]# curl -I 192.168.0.10:443\u0015\u0003\u0001\u0002\u0002注意事项将kube-apiserver配置成高可用后，需要将kube-controller-manager和kube-scheduler配置文件中--master选项对应的地址改成vip的地址，然后重启这两个服务。部署Node节点绑定集群角色将kubelet-bootstrap用户绑定到系统集群角色12[root@k8s-master1 ~]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created创建kubeconfig文件在生成kubernetes证书的目录下执行以下命令生成kubeconfig文件：1234567891011121314151617181920212223242526## 创建kubelet bootstrapping kubeconfig [root@k8s-master1 ~]# cd /home/kubernetes/ssl# 指定apiserver 内网负载均衡地址[root@k8s-master1 ~]# export KUBE_APISERVER=&quot;https://192.168.0.10:6443&quot;[root@k8s-master1 ~]# export BOOTSTRAP_TOKEN=29cd86c2fd73035417e38c53bc8f12d9# 设置集群参数[root@k8s-master1 ~]# kubectl config set-cluster kubernetes \\--certificate-authority=./ca.pem \\--embed-certs=true \\--server=$&#123;KUBE_APISERVER&#125; \\--kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数[root@k8s-master1 ~]# kubectl config set-credentials kubelet-bootstrap \\--token=$&#123;BOOTSTRAP_TOKEN&#125; \\--kubeconfig=bootstrap.kubeconfig# 设置上下文参数[root@k8s-master1 ~]# kubectl config set-context default \\--cluster=kubernetes \\--user=kubelet-bootstrap \\--kubeconfig=bootstrap.kubeconfig# 设置默认上下文[root@k8s-master1 ~]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig创建kube-proxy kubeconfig文件123456789101112131415161718192021[root@k8s-master1 ~]# kubectl config set-cluster kubernetes \\--certificate-authority=./ca.pem \\--embed-certs=true \\--server=$&#123;KUBE_APISERVER&#125; \\--kubeconfig=kube-proxy.kubeconfig[root@k8s-master1 ~]# kubectl config set-credentials kube-proxy \\--client-certificate=./kube-proxy.pem \\--client-key=./kube-proxy-key.pem \\--embed-certs=true \\--kubeconfig=kube-proxy.kubeconfig[root@k8s-master1 ~]# kubectl config set-context default \\--cluster=kubernetes \\--user=kube-proxy \\--kubeconfig=kube-proxy.kubeconfig[root@k8s-master1 ~]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig[root@k8s-master1 ~]# ls *.kubeconfigbootstrap.kubeconfig kube-proxy.kubeconfig将这两个配置文件拷贝到node节点的/home/kubernetes/conf/目录下12[root@k8s-master1 ~]# scp *.kubeconfig k8s-node1:/home/kubernetes/conf/[root@k8s-master1 ~]# scp *.kubeconfig k8s-node2:/home/kubernetes/conf/部署kubelet组件将前面下载二进制包中的kubelet和kube-proxy拷贝到Node节点的/home/kubernetes/bin目录下123[root@k8s-master1 ~]# cd kubernetes/server/bin/[root@k8s-master1 bin]# scp kubelet kube-proxy k8s-node1:/home/kubernetes/bin/[root@k8s-master1 bin]# scp kubelet kube-proxy k8s-node2:/home/kubernetes/bin/创建kubelet配置文件123456789[root@k8s-node1 ~]# vim /home/kubernetes/conf/kubeletKUBELET_OPTS=&quot;--logtostderr=true \\--v=4 \\--hostname-override=192.168.0.13 \\--kubeconfig=/home/kubernetes/conf/kubelet.kubeconfig \\--bootstrap-kubeconfig=/home/kubernetes/conf/bootstrap.kubeconfig \\--config=/home/kubernetes/conf/kubelet.config \\--cert-dir=/home/kubernetes/ssl \\--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot;参数说明：--hostname-override 在集群中显示的主机名--kubeconfig 指定kubeconfig文件位置，会自动生成--bootstrap-kubeconfig 指定刚才生成的bootstrap.kubeconfig文件--cert-dir 颁发证书存放位置--cluster-dns 集群DNS IP，先配置上，后面会讲到--cluster-domain DNS域--fail-swap-on=false 禁止使用swap--pod-infra-container-image 管理Pod网络的镜像编写kubelet.config配置文件:123456789101112131415[root@k8s-node1 ~]# vim /home/kubernetes/conf/kubelet.configkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 192.168.0.13port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [&quot;192.168.0.2&quot;]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true webhook: enabled: false使用systemctl管理kubelet组件：1234567891011121314[root@k8s-node1 ~]# vim /usr/lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=/home/kubernetes/conf/kubeletExecStart=/home/kubernetes/bin/kubelet $KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.target启动kubelet:12systemctl start kubeletsystemctl enable kubelet在master审批Node加入集群1234567891011121314151617181920212223# 查看请求签名的node[root@k8s-master1 ssl]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-WshP4cKZMpaIT8mE67wfk2PE1LKl2tz_6-KbMcCNL5M 2d kubelet-bootstrap Pendingnode-csr-zvLNjTqRalExD0Mhr0SYiZ83uSZ0uSm0kNvoaix56hc 2d kubelet-bootstrap Pending# 通过CSR 请求[root@k8s-master1 ~]# kubectl certificate approve node-csr-WshP4cKZMpaIT8mE67wfk2PE1LKl2tz_6-KbMcCNL5Mcertificatesigningrequest.certificates.k8s.io/node-csr-WshP4cKZMpaIT8mE67wfk2PE1LKl2tz_6-KbMcCNL5M approved[root@k8s-master1 ~]# kubectl certificate approve node-csr-zvLNjTqRalExD0Mhr0SYiZ83uSZ0uSm0kNvoaix56hccertificatesigningrequest.certificates.k8s.io/node-csr-zvLNjTqRalExD0Mhr0SYiZ83uSZ0uSm0kNvoaix56hc approved# 查看请求签名的node状态[root@k8s-master1 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-WshP4cKZMpaIT8mE67wfk2PE1LKl2tz_6-KbMcCNL5M 2d kubelet-bootstrap Approved,Issuednode-csr-zvLNjTqRalExD0Mhr0SYiZ83uSZ0uSm0kNvoaix56hc 2d kubelet-bootstrap Approved,Issued# 查看node连接状态[root@k8s-master1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSION192.168.0.13 Ready &lt;none&gt; 42m v1.11.5192.168.0.14 Ready &lt;none&gt; 28s v1.11.5部署kube-proxy组件创建kube-proxy配置文件123456[root@k8s-node1 ~]# vim /home/kubernetes/conf/kube-proxyKUBE_PROXY_OPTS=&quot;--logtostderr=true \\--v=4 \\--hostname-override=192.168.0.13 \\--cluster-cidr=192.168.0.0/24 \\--kubeconfig=/home/kubernetes/conf/kube-proxy.kubeconfig&quot;systemctl管理kube-proxy组件123456789101112[root@k8s-node1 ~]# vim /usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=-/home/kubernetes/conf/kube-proxyExecStart=/home/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTSRestart=on-failure[Install]WantedBy=multi-user.target启动kube-proxy123[root@k8s-node1 ~]# systemctl enable kube-proxy[root@k8s-node1 ~]# systemctl start kube-proxy[root@k8s-node1 ~]# systemctl status kube-proxy6.6 安装完所有服务后，检查一下集群状态12345678910111213141516[root@k8s-master1 ~]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-3 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; [root@k8s-master1 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-WshP4cKZMpaIT8mE67wfk2PE1LKl2tz_6-KbMcCNL5M 2d kubelet-bootstrap Approved,Issuednode-csr-zvLNjTqRalExD0Mhr0SYiZ83uSZ0uSm0kNvoaix56hc 2d kubelet-bootstrap Approved,Issued[root@k8s-master1 ~]# kubectl get nodeNAME STATUS ROLES AGE VERSION192.168.0.13 Ready &lt;none&gt; 1h v1.11.5192.168.0.14 Ready &lt;none&gt; 24m v1.11.5运行一个测试示例创建一个nginx，判断集群是否正常工作1234[root@k8s-master1 ~]# kubectl run nginx --image=nginx --replicas=3deployment.apps/nginx created[root@k8s-master1 ~]# kubectl expose deployment nginx --port=88 --target-port=80 --type=NodePortservice/nginx exposed查看已创建的pod123456789[root@k8s-master1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-64f497f8fd-6gg5n 0/1 ContainerCreating 0 23snginx-64f497f8fd-qj8p9 0/1 ContainerCreating 0 23snginx-64f497f8fd-xxmtm 0/1 ContainerCreating 0 23s[root@k8s-master1 ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 192.168.0.1 &lt;none&gt; 443/TCP 2dnginx NodePort 192.168.0.119 &lt;none&gt; 88:48065/TCP 25s访问node节点的ip的48065端口(http://192.168.0.13:48065)看到如上页面表示集群搭建成功安装kubernetes-dashboard创建kubernetes-dashboardyaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394[root@k8s-master1 ~]# mkdir /home/dashboard[root@k8s-master1 ~]# cd /home/dashboard/[root@k8s-master1 dashboard]# vim dashboard-deployment.yaml apiVersion: apps/v1beta2kind: Deploymentmetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos; spec: serviceAccountName: kubernetes-dashboard containers: - name: kubernetes-dashboard image: registry.cn-hangzhou.aliyuncs.com/kube_containers/kubernetes-dashboard-amd64:v1.8.1 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 100Mi ports: - containerPort: 9090 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 tolerations: - key: &quot;CriticalAddonsOnly&quot; operator: &quot;Exists&quot;[root@k8s-master1 dashboard]# vim dashboard-rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: Reconcile name: kubernetes-dashboard namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: kubernetes-dashboard-minimal namespace: kube-system labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: ReconcileroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system[root@k8s-master1 dashboard]# vim dashboard-service.yaml apiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 80 targetPort: 9090运行kubernetes-dashboard1[root@k8s-master1 dashboard]# kubectl create -f .查看访问端口:123[root@k8s-master1 ~]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 192.168.0.15 &lt;none&gt; 80:30287/TCP 5m排错方法：12345678# 查看pod状态[root@k8s-master1 ~]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGEkubernetes-dashboard-d9545b947-dk2qg 0/1 CrashLoopBackOff 3 1m# 查看pod详细信息[root@k8s-master1 ~]# kubectl describe pod kubernetes-dashboard -n kube-system# 查看pod日志信息[root@k8s-master1 ~]# kubectl logs kubernetes-dashboard-d9545b947-dk2qg -n kube-system","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/categories/K8s/"},{"name":"Docker","slug":"K8s/Docker","permalink":"http://blog.wubolive.com/categories/K8s/Docker/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/tags/K8s/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.wubolive.com/tags/Docker/"}]},{"title":"CentOS6 安装nfs共享目录服务","slug":"nfs-install","date":"2018-12-07T09:13:28.000Z","updated":"2018-12-07T09:15:50.636Z","comments":true,"path":"2018/12/07/nfs-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/07/nfs-install/","excerpt":"","text":"简介服务器IP：172.17.0.3客户端IP：172.17.0.10共享目录：/home/data服务端安装1[root@CentOS ~]# yum -y install nfs-utils rpcbind配置防火墙关闭selinux共享的目录：/home/data配置指定共享目录及主机12[root@CentOS ~]# vim /etc/exports/home/data 172.17.0.10(rw,no_root_squash,no_all_squash,sync)启动nfs及rpcbind1234[root@CentOS ~]# chkconfig nfs on[root@CentOS ~]# chkconfig rpcbind on[root@CentOS ~]# service nfs start[root@CentOS ~]# service rpcbind start客户端安装1[root@CentOS ~]# yum -y install nfs-utils查看服务端共享信息123[root@CentOS ~]# showmount -e 172.17.0.3Export list for 172.17.0.3:/home/data 172.17.0.10挂载nfs共享目录123[root@CentOS ~]# mkdir -p /home/data#挂载nfs,格式(mount -t nfs 服务器IP:/服务器目录 /本机挂载点)[root@CentOS ~]# mount -t nfs 172.17.0.3:/home/data /home/data配置开机自动挂载123[root@CentOS ~]# vim /etc/fstab在文件最后加入下行172.17.0.3:/home/data /home/data nfs defaults 0 0卸载共享目录1[root@CentOS ~]# umount 172.17.0.3:/home/data##nfs挂载出现nobody解决方法1mount -t nfs4 -o vers=3 ip:/pwd /pwd","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"NFS","slug":"NFS","permalink":"http://blog.wubolive.com/tags/NFS/"}]},{"title":"vncserver端口占用解决方法","slug":"vnc-pord","date":"2018-12-07T09:00:28.000Z","updated":"2018-12-07T09:18:42.748Z","comments":true,"path":"2018/12/07/vnc-pord/","link":"","permalink":"http://blog.wubolive.com/2018/12/07/vnc-pord/","excerpt":"","text":"在重启kvn物理服务器后启动vncserver服务时报错，如下123[root@node5 ~]# service vncserver startStarting VNC server: 1:root A VNC server is already running as :1 [FAILED]使用lsof命令查看5902端口，发现被kvn实例占用了12345678[root@node5 ~]# /usr/sbin/lsof -i tcp:5902COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEqemu-kvm 4214 qemu 15u IPv4 19697 0t0 TCP *:5902 (LISTEN)[root@node5 ~]# netstat -lntp | grep 5902 tcp 0 0 0.0.0.0:5902 0.0.0.0:* LISTEN 4214/qemu-kvm[root@node5 ~]# ps -ef |grep 4214qemu 4214 1 1 Sep23 ? 01:12:38 /usr/libexec/qemu-kvm -name node6-vm29.xxzx.local -S -M rhel6.6.0 -enable-kvm -m 8196 -realtime mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid ......省略数行......进过百度发现vncserver端口可以是5901,5902,5903…12345[root@node5 ~]# netstat -lntp | grep 590tcp 0 0 0.0.0.0:5900 0.0.0.0:* LISTEN 4124/qemu-kvm tcp 0 0 0.0.0.0:5901 0.0.0.0:* LISTEN 4175/qemu-kvm tcp 0 0 0.0.0.0:5902 0.0.0.0:* LISTEN 4214/qemu-kvm tcp 0 0 0.0.0.0:5903 0.0.0.0:* LISTEN 5799/qemu-kvm配置/etc/sysconfig/vncservers文件修改vncserver端口12345[root@node5 ~]# vim /etc/sysconfig/vncservers# 将下面数字配置成端口未被占用的尾数(比如说5904未被占用，则修改成4:root)VNCSERVERS=&quot;4:root&quot;VNCSERVERARGS[4]=&quot;-geometry 1280x800&quot;# 1280x800表示vnc客户端连接时的分辨率最后启动vncserver服务测试是否成功1234567891011121314[root@node5 ~]# service vncserver start Starting VNC server: 4:root New &apos;node5:4 (root)&apos; desktop is node5:4Starting applications specified in /root/.vnc/xstartupLog file is /root/.vnc/node5:4.log [ OK ][root@node5 ~]# vncserver -listTigerVNC server sessions:X DISPLAY # PROCESS ID:4 19336","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"Vnc","slug":"Vnc","permalink":"http://blog.wubolive.com/tags/Vnc/"}]},{"title":"rabbitMQ 安装","slug":"rebbitmq-install","date":"2018-12-07T07:00:28.000Z","updated":"2018-12-07T09:22:17.535Z","comments":true,"path":"2018/12/07/rebbitmq-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/07/rebbitmq-install/","excerpt":"","text":"安装依赖12yum install epel-releaseyum install erlang下载rpm包1wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.15/rabbitmq-server-3.6.15-1.el7.noarch.rpm安装rabbitmq-server1yum -y install rabbitmq-server-3.6.15-1.el7.noarch.rpm启动12systemctl start rabbitmq-serversystemctl enable rabbitmq-server查看当前所有用户1rabbitmqctl list_users查看默认guest用户的权限1rabbitmqctl list_user_permissions guest由于RabbitMQ默认的账号用户名和密码都是guest。为了安全起见, 先删掉默认用户1rabbitmqctl delete_user guest###添加新用户1rabbitmqctl add_user [admin] [admin123]设置用户tag1rabbitmqctl set_user_tags [admin] administrator赋予用户默认vhost的全部操作权限1rabbitmqctl set_permissions -p / [admin] &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;查看用户的权限1rabbitmqctl list_user_permissions [admin]开启web端口1rabbitmq-plugins enable rabbitmq_management","categories":[{"name":"MQ","slug":"MQ","permalink":"http://blog.wubolive.com/categories/MQ/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"rabbitMQ","slug":"rabbitMQ","permalink":"http://blog.wubolive.com/tags/rabbitMQ/"}]},{"title":"vncserver安装及配置","slug":"vnc-install","date":"2018-12-07T07:00:28.000Z","updated":"2018-12-07T09:23:08.607Z","comments":true,"path":"2018/12/07/vnc-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/07/vnc-install/","excerpt":"","text":"##说明安装服务的过程当中，最好是在联网环境下操作。由于涉及到远程连接的问题，系统的防火墙需要关闭，或者是做好相应的过滤策略。参考了网上很多大牛的意见和文章，结合自己的安装经历，现在总结一下。##1.下载并按照vnc-server###1.1 检查是否已经安装默认情况下，vnc服务是没有被系统安装的，可以通过以下命令查看：123[root@centos6 ~]# rpm -qa | grep vnctigervnc-server-1.1.0-24.el6.x86_64tigervnc-1.1.0-24.el6.x86_64执行查询命令，如果没有内容说明还没有安装vnc服务，需要安装。###1.2 下载和安装其实这个步骤很简单，都交给系统去做了，执行下面yum安装命令即可完成：1[root@centos6 ~]# yum install tigervnc tigervnc-server -y##2.配置vncserver打开配置文件1234[root@centos6 ~]# vim /etc/sysconfig/vncserver#在文件修改为下面两行代码：VNCSERVERS=&quot;1:root&quot;VNCSERVERARGS[1]=&quot;-geometry 1024x768 -nolisten tcp -localhost&quot;##3.设置或修改vnc登录密码配置当前用户的vnc登陆密码，连续输入两次即可。修改vnc的登录密码，也是和设置新密码一样的步骤。这个密码是vnc连接的独立密码，与系统密码没有关系。123[root@centos6 ~]# vncpasswd Password:Verify:##4.配置防火墙规则，允许vnc远程连接VNC server监听的端口从5900开始，display:1的监听 5901，display:2监听 5902，以此类推。CentOS 的防火墙缺省是不允许连接这些端口的，需要配置防火墙开放相关端口（root 权限)1234567[root@master /]# vi /etc/sysconfig/iptables...#开放 &quot;5902&quot; 端口...-A INPUT -m state --state NEW -m tcp -p tcp --dport 5901 -j ACCEPT #开放 &quot;5901&quot; 端口-A INPUT -m state --state NEW -m tcp -p tcp --dport 5902 -j ACCEPT #重启防火墙[root@master /]# service iptables restart##5.启动服务（开启多个远程端口）并设置开机自启动###5.1 启动方式一让系统自动分配连接端口：123456789[root@centos6 ~]# vncserver xauth: file /root/.Xauthority does not existxauth: (stdin):1: bad display name &quot;centos6.5-1:1&quot; in &quot;add&quot; commandNew &apos;centos6.5-1:1 (root)&apos; desktop is centos6.5-1:1Creating default startup script /root/.vnc/xstartupStarting applications specified in /root/.vnc/xstartupLog file is /root/.vnc/centos6.5-1:1.log直接输入则会自动打开一个连接服务“centos6.5-1:1”说明这是启动的第一个服务###5.2 启动方式二指定端口启动，如果指定端口的vnc服务已经启动则会提示，如果还没有启动则会重新启动特定端口的vnc服务：12[root@centos6 ~]# vncserver :1 #指定端口1（即系统的5901端口）启动服务，注意“:1”之前有一个空格[root@centos6 ~]# vncserver :2 #指定端口2（即系统的5902端口）启动服务###5.3 开机自启动设置开机自动启动服务：123[root@centos6 ~]# chkconfig vncserver on[root@centos6 ~]# chkconfig --list | grep vncservervncserver 0:off 1:off 2:on 3:on 4:on 5:on 6:off##6.windows环境远程连接服务器###6.1 客户端远程连接下载且安装好windows桌面的vnc-viewer客户端；然后在VNC Server（服务器）这一项输入“目标主机IP:登录端口（没错，端口就是上面的1或者2或3等）”；Encryption（加密）这一项采用默认方式（让vnc server自己选择）；点击“Connection（连接）”；输入设置的vnc登录密码（注意是vnc的密码，不是目标主机系统的登录密码）。###6.2 使用浏览器远程连接可以参考如下链接Chronme VNC远程连接插件##7.关闭vncserver执行kill命令：12345678910# 查看开启的vnc端口[root@master /]# vncserver -listTigerVNC server sessions:X DISPLAY # PROCESS ID:1 19558:2 20069:3 20447[root@master /]# vncserver -kill :1 #关闭端口1[root@master /]# vncserver -kill :2 #关闭端口2注意：-kill与:1或:2中间有一空格","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"Vnc","slug":"Vnc","permalink":"http://blog.wubolive.com/tags/Vnc/"}]},{"title":"Harbr更改端口","slug":"k8s-harbor-port","date":"2018-12-04T02:36:28.000Z","updated":"2018-12-04T01:12:07.205Z","comments":true,"path":"2018/12/04/k8s-harbor-port/","link":"","permalink":"http://blog.wubolive.com/2018/12/04/k8s-harbor-port/","excerpt":"","text":"前言默认情况下，Harbor的侦听端口80(HTTP)和443(HTTPS)，因某原因无法使用80或443端口可按照本方法更改。修改docker-compose.yml文件找到nginx镜像配置,将外网映射端口改成未被占用的端口123456789101112131415161718[root@hub harbor]# vim docker-compose.yml proxy: image: goharbor/nginx-photon:v1.6.2 container_name: nginx restart: always volumes: - ./common/config/nginx:/etc/nginx:z networks: - harbor ports: - 88:80 - 843:443 - 8443:4443 depends_on: - postgresql - registry - ui - log更改harbor.cfg文件将hostname添加上映射端口12[root@hub harbor]# vim harbor.cfg hostname = hub.wubolive.com:88重启docker-compose12[root@hub harbor]# docker-compose down -v[root@hub harbor]# ./install","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/categories/K8s/"},{"name":"Docker","slug":"K8s/Docker","permalink":"http://blog.wubolive.com/categories/K8s/Docker/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/tags/K8s/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.wubolive.com/tags/Docker/"}]},{"title":"Harbor私有仓库安装","slug":"k8s-harbor-install","date":"2018-12-04T01:11:28.000Z","updated":"2018-12-04T01:11:55.201Z","comments":true,"path":"2018/12/04/k8s-harbor-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/04/k8s-harbor-install/","excerpt":"","text":"Harbor安装https://github.com/vmware/harbor/releases 中下载Harbor online installer离线安装包123[root@hub ~]# wget https://storage.googleapis.com/harbor-releases/release-1.6.0/harbor-online-installer-v1.6.2.tgz[root@hub ~]# tar zxf harbor-online-installer-v1.6.2.tgz [root@hub ~]# cd harbor/配置harbor如果不想已80端口运行，请查看：http://www.wubolive.com/post/harbor-port123456789#更改映射端口可修改[root@hub harbor]# vim harbor.cfghostname = hub.wubolive.com #配置访问地址ui_url_protocol = http harbor_admin_password = 123456 #Web登录页面密码# 准备配置文件[root@hub harbor]# ./prepare# 安装启动harbor[root@hub harbor]# ./install.sh查看以运行的docker容器12345678910[root@hub harbor]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc39813d51868 goharbor/nginx-photon:v1.6.2 &quot;nginx -g &apos;daemon of…&quot; 3 minutes ago Up 3 minutes (healthy) 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp nginxc80e7a781c7a goharbor/harbor-jobservice:v1.6.2 &quot;/harbor/start.sh&quot; 3 minutes ago Up 3 minutes harbor-jobservice189ed8429b30 goharbor/harbor-ui:v1.6.2 &quot;/harbor/start.sh&quot; 3 minutes ago Up 3 minutes (healthy) harbor-ui99774d0dd424 goharbor/redis-photon:v1.6.2 &quot;docker-entrypoint.s…&quot; 3 minutes ago Up 3 minutes 6379/tcp redis9eead178687e goharbor/harbor-db:v1.6.2 &quot;/entrypoint.sh post…&quot; 3 minutes ago Up 3 minutes (healthy) 5432/tcp harbor-db82a2cfce37ee goharbor/harbor-adminserver:v1.6.2 &quot;/harbor/start.sh&quot; 3 minutes ago Up 3 minutes (healthy) harbor-adminserver9314f9963d75 goharbor/registry-photon:v2.6.2-v1.6.2 &quot;/entrypoint.sh /etc…&quot; 3 minutes ago Up 3 minutes (healthy) 5000/tcp registry3cdab5ca0e61 goharbor/harbor-log:v1.6.2 &quot;/bin/sh -c /usr/loc…&quot; 3 minutes ago Up 3 minutes (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-log访问Harbor浏览器输入：http://hub.wubolive.com登入账号：admin登入密码：123456在面板中创建一个用户K8S并加入library项目上传docker镜像因docker默认不支持http方式上传镜像，所有需要在配置文件中指明使用http访问12345[root@hub harbor]# vim /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;http://cc51a693.m.daocloud.io&quot;], &quot;insecure-registries&quot;: [&quot;hub.wubolive.com&quot;]&#125;登入Harbor12345[root@hub harbor]# docker login hub.wubolive.comUsername: k8sPassword: Login Succeeded# 显示Login Succeeded说明登入成功推送镜像格式：1234# docker tag 镜像中心域名/项目名称/镜像名:版本docker tag SOURCE_IMAGE[:TAG] hub.wubolive.com/library/IMAGE[:TAG]# docker push 更改tag后的镜像名docker push hub.wubolive.com/library/IMAGE[:TAG]拉取镜像格式：1docker pull hub.wubolive.com/library/nginx:latest附：以HTTPS方式部署Harborhttps://github.com/goharbor/harbor/blob/master/docs/configure_https.md","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/categories/K8s/"},{"name":"Docker","slug":"K8s/Docker","permalink":"http://blog.wubolive.com/categories/K8s/Docker/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/tags/K8s/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.wubolive.com/tags/Docker/"}]},{"title":"Kvm虚拟机迁移方案","slug":"kvm-scp","date":"2018-12-01T02:00:28.000Z","updated":"2018-12-07T09:25:22.014Z","comments":true,"path":"2018/12/01/kvm-scp/","link":"","permalink":"http://blog.wubolive.com/2018/12/01/kvm-scp/","excerpt":"","text":"在node1上的img镜像文件拷贝到node11上1[root@node1 ~]# scp /home/mv/vm01.centos.local.img root@172.17.0.12:/home/vm在node1上执行迁移命令（源主机和目标主机存放镜像的路径必须保持一致，否则报错）123[root@node1 ~]# virsh migrate --live --verbose vm01.centos.local qemu+ssh://172.17.0.12/system tcp://172.17.0.12root@172.17.0.12&apos;s password: Migration: [100 %]###在迁移过程中在本机一直ping正在迁移的虚拟机1C:\\Users\\Administrator&gt;ping -t 172.17.0.55中间会有短暂的延迟，不过妨碍不大到node11上生成虚拟机配置文件1[root@node11 ~]# virsh dumpxml vm01.centos.local &gt; /etc/libvirt/qemu/vm01.centos.local.xml###通过迁移后生成的配置文件从新定义虚拟机` [root@node11 ~]# virsh define /etc/libvirt/qemu/vm01.centos.local.xml Domain vm01.centos.local defined from /etc/libvirt/qemu/vm01.centos.local.xml","categories":[{"name":"Kvm","slug":"Kvm","permalink":"http://blog.wubolive.com/categories/Kvm/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"Kvm","slug":"Kvm","permalink":"http://blog.wubolive.com/tags/Kvm/"}]},{"title":"Linux以虚拟用户方式安装Vsftpd","slug":"vsftpd-install","date":"2018-11-30T06:36:28.000Z","updated":"2018-12-04T01:07:20.857Z","comments":true,"path":"2018/11/30/vsftpd-install/","link":"","permalink":"http://blog.wubolive.com/2018/11/30/vsftpd-install/","excerpt":"","text":"说明：vsftpd的版本：vsftpd-3.0.2-22.el7.x86_64ftp 根目录 : /data/ftpftp 配置文件目录：/etc/vsftpdftp 虚拟用户权限配置文件目录：/etc/vsftpd/vuser_config实现目标：匿名用户可以登录，但是不能访问虚拟用户的宿主目录，只能访问共享目录虚拟用户对自己的宿主目录有任何权限，且只能在自己宿主目录中操作搭建过程1.安装vsftpd，ftp和libdb-utils(需要安装db包，用来加密虚拟用户的账户信息,centos7默认安装好了)1[root@CentOS ~]# yum install vsftpd ftp -y2.创建本地用户[用于映射虚拟用户]1234567#建立ftp用户目录[root@CentOS ~]# mkdir -p /ftp-dir#创建用户[root@CentOS ~]# useradd -d /ftp-dir/ vftpuser -s /sbin/nologin #更改权限和主组权限[root@CentOS ~]# chmod 755 /ftp-dir[root@CentOS ~]# chown vftpuser.root /ftp-dir3.创建虚拟用户[用户和密码]文件12345[root@CentOS ~]# vim /etc/vsftpd/vuseradmin [用户名]123456 [密码]devops [用户名]123456 [密码]4.加密用户密码文件生成数据库文件123[root@CentOS ~]# cd /etc/vsftpd/[root@CentOS ~]# db_load -T -t hash -f ./vuser ./login.db[root@CentOS ~]# chmod 600 login.db5.创建PAM认证文件1234[root@CentOS ~]# vim /etc/pam.d/vsftpd.vuauth required /lib64/security/pam_userdb.so db=/etc/vsftpd/login #注意64位系统写/lib64这个路径，32位系统要写成/lib，下同！account required /lib64/security/pam_userdb.so db=/etc/vsftpd/login6.修改配置文件1234567891011121314151617181920212223242526272829303132[root@CentOS ~]# vi /etc/vsftpd/vsftpd.conf#允许匿名用户访问anonymous_enable=yeslocal_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_file=/var/log/xferlogxferlog_std_format=YESlisten=NOlisten_ipv6=YES#修改ftp默认目录到/ftp-dir下面chroot_local_user=YESlocal_root=/ftp-diranon_root=/ftp-dir#pam认证文件pam_service_name=vsftpd.vu#虚拟用户权限配置目录user_config_dir=/etc/vsftpd/ftploginuserlist_enable=YEStcp_wrappers=YESallow_writeable_chroot=YESone_process_model=NO#开启虚拟用户guest_enable=YESguest_username=vftpuser注意，如果vsftpd的版本是：vsftpd-2.2.2-24.el6.x86_64那么vsftpd.conf的配置文件修改如下,其他操作一样，不然的话2.2.2版本启动时会遇到各种问题：123456789101112131415161718listen=YESlocal_enable=YESanon_umask=022anonymous_enable=YESpam_service_name=vsftpd.vuuserlist_enable=YESchroot_local_user=YESlocal_root=/ftp-diranon_root=/ftp-dirguest_enable=YESguest_username=vftpuseruser_config_dir=/etc/vsftpd/ftploginuserlist_enable=YESxferlog_enable=YESxferlog_std_format=YESxferlog_file=/var/log/xferlogdual_log_enable=YESvsftpd_log_file=/var/log/vsftpd.log7.重启vsftpd服务1[root@CentOS ~]# systemctl restart vsftpd8.创建虚拟用户[权限]配置文件123456789101112131415[root@CentOS ~]# mkdir /etc/vsftpd/ftplogin[root@CentOS ~]# cd /etc/vsftpd/ftplogin[root@CentOS ~]# vi abc [有所有权限]#设置登录后禁锢的目录local_root=/ftp-dir/admin#开放写权限write_enable=yes#开放下载权限anon_world_readable_only=no#开放上传权限anon_upload_enable=yes#开放创建目录的权限anon_mkdir_write_enable=yes#开放删除和重命名的权限anon_other_write_enable=yes1234# vi bcd [只有上传下载的权限]local_root=/ftp-dir/devopsanon_upload_enable=yesanon_world_readable_only=no9.更改虚拟用户目录权限1234#如果不更改的话，匿名用户是可以访问到的[root@CentOS ~]# mkdir /ftp-dir/admin &amp;&amp; chmod 700 /ftp-dir/admin[root@CentOS ~]# mkdir /ftp-dir/devops &amp;&amp; chmod 700 /ftp-dir/devops[root@CentOS ~]# chown -R vftpuser.root /ftp-dir10.测试访问。1234567891011[root@CentOS ~]# ftp 127.0.0.1Connected to 127.0.0.1 (127.0.0.1).220 (vsFTPd 3.0.2)Name (127.0.0.1:root): devops331 Please specify the password.Password:230 Login successful.Remote system type is UNIX.Using binary mode to transfer files.ftp&gt; exit221 Goodbye.11.添加用户(不需要重启vsftpd服务)1234567891011121314#添加新用户test，密码为abcd[root@CentOS ~]# vim /etc/vsftpd/vuseradmin123456devops123456user1123456#创建test用户对应的目录并授权[root@CentOS ~]# mkdir /ftp-dir/user1 &amp;&amp; chown vuser:root /ftp-dir/user1#重新生成加密的db文件[root@CentOS ~]# cd /etc/vsftpd[root@CentOS ~]# db_load -T -t hash -f ./vuser ./login.db#删除用户就是把添加用户的操作撤销，然后删除加密的db文件重新生成即可。12.测试访问，此时添加的test用户的/ftp-dir/test目录的权限是755，匿名用户可以登录。若要屏蔽，修改权限为700即可。1[root@CentOS ~]# ftp 127.0.0.113.错误处理错误1：226 Transfer done (but failed to open directory)解决：selinux 和防火墙导致错误2：500 OOPS: vsftpd: refusing to run with writable root inside chroot()解决：配置文件中加入 allow_writeable_chroot=YES 针对标准vsftpd(standonly)模式，然后重启ftp.vsftp上传文件权限问题file_open_mode上传档案的权限，与chmod 所使用的数值相同。如果希望上传的文件可以执行，设此值为0777。默认情况下vsftp上传之后文件的权限是600，目录权限是700local_umask=xxx这是指定本地用户上传后的文件权限设置anon_umask=xxx这是指定虚拟用户上传后的文件权限设置umask是unix操作系统的概念，umask决定目录和文件被创建时得到的初始权限umask = 022时，新建的目录 权限是755，文件的权限是 644umask = 077时，新建的目录 权限是700，文件的权限时 600","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"FTP","slug":"FTP","permalink":"http://blog.wubolive.com/tags/FTP/"}]},{"title":"Ansible Ad-Hoc命令集","slug":"ansible-hoc","date":"2018-11-30T06:13:28.000Z","updated":"2018-11-30T13:43:04.902Z","comments":true,"path":"2018/11/30/ansible-hoc/","link":"","permalink":"http://blog.wubolive.com/2018/11/30/ansible-hoc/","excerpt":"","text":"ansible命令使用方法123456789101112131415161718192021222324ansible &lt;host-pattern&gt; [options]可用选项如下。 -v,--verbose:输出更详细的执行过程信息,-vvv可得到执行过程所有信息。 -i PATH,--inventory=PATH:指定inventory信息,默认/etc/absible/hosts。 -f NUM,--forks=NUM:并发线程数,默认5个线程。 --private-key=PRIVATE_KEY_FILE:指定密钥文件。 -m NAME,--module-name=NAME:指定执行使用的模块。 -M DIRECTORY,--module-path=DIRECTORY:指定模块存放路径,默认/usr/share/ansible,也可以通过ANSIBLE_LIBRARY设定默认路径。 -a &apos;ARGUMENTS&apos;,--args=&apos;ARGUMENTS&apos;:模块参数。 -k,--ask-pass SSH:认证密码。 -K,--ask-sudo-pass sudo:用户的密码(--sudo时使用)。 -o,--one-line:标准输出至一行。 -s,--sudo:相当于Linux系统下的sudo命令。 -t DIRECTORY,--tree=DIRECTORY:输出信息至DIRECTORY目录下,结果文件以远程主机名命名。 -T SECONDS,--timeout=SECONDS:指定连接远程主机的最大超时,单位是秒。 -B NUM,--background=NUM:后台执行命令,超NUM秒后中止正在执行的任务。 -P NUM,--poll=NUM:定期返回后台任务进度。 -u USERNAME,--user=USERNAME:指定远程主机以USERNAME运行命令。 -U SUDO_USERNAME,--sudo-user=SUDO_USERNAME:使用sudo,相当于Linux下的sudo命令。 -c CONNECTION,--connection=CONNECTION:指定连接方式,可用选项paramiko(SSH)、ssh、local,local方式常 用于crontab和kickstarts。 -l SUBSET,--limit=SUBSET:指定运行主机。 -l ~REGEX,--limit=~REGEX:指定运行主机(正则)。 --list-hosts:列出符合条件的主机列表,不执行任何命令。情景1:检查all组所有主机是否存活12345678910111213[root@Ansible ~]# ansible all -f 5 -m ping36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;情景2:返回all组所有主机的hostname123456789[root@Ansible ~]# ansible all -s -m command -a &apos;hostname&apos; 36.103.245.138 | SUCCESS | rc=0 &gt;&gt;node136.103.245.70 | SUCCESS | rc=0 &gt;&gt;node236.103.245.135 | SUCCESS | rc=0 &gt;&gt;master流程图情景3：列出all组所有主机列表12345[root@Ansible ~]# ansible all --list hosts (3): 36.103.245.135 36.103.245.138 36.103.245.70情景4：列出all组所有主机磁盘使用情况1234567891011121314151617181920212223[root@Ansible ~]# ansible all -a &apos;df -lh&apos;36.103.245.138 | SUCCESS | rc=0 &gt;&gt;Filesystem Size Used Avail Use% Mounted ondevtmpfs 7.9G 0 7.9G 0% /devtmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 7.9G 57M 7.8G 1% /run/dev/vda2 36G 2.8G 31G 9% /36.103.245.70 | SUCCESS | rc=0 &gt;&gt;Filesystem Size Used Avail Use% Mounted ondevtmpfs 7.9G 0 7.9G 0% /devtmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 7.9G 57M 7.8G 1% /run/dev/vda2 36G 2.8G 31G 9% //dev/vdb1 99G 61M 94G 1% /home36.103.245.135 | SUCCESS | rc=0 &gt;&gt;Filesystem Size Used Avail Use% Mounted ondevtmpfs 7.9G 0 7.9G 0% /devtmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 7.9G 185M 7.7G 3% /run/dev/vda2 36G 25G 9.4G 72% //dev/vdb1 99G 61M 94G 1% /home情景5：列出all组所有主机内存使用情况123456789101112131415[root@Ansible ~]# ansible all -m shell -a &quot;free -h&quot;36.103.245.138 | SUCCESS | rc=0 &gt;&gt; total used free shared buff/cache availableMem: 15G 384M 13G 56M 1.4G 14GSwap: 0B 0B 0B36.103.245.70 | SUCCESS | rc=0 &gt;&gt; total used free shared buff/cache availableMem: 15G 377M 13G 56M 1.3G 14GSwap: 0B 0B 0B36.103.245.135 | SUCCESS | rc=0 &gt;&gt; total used free shared buff/cache availableMem: 15G 839M 11G 184M 3.1G 14GSwap: 0B 0B 0Bansible-doc命令使用方法123456789ansible-doc [options] [module...]可用选项如下。 --version:显示工具版本号 -h,--help:显示该help说明 -M MODULE_PATH,--module-path=MODULE_PATH:指定Ansible模块的默认加载目录。 -l,--list:列出所有可用的模块。 -s,--snippet:只显示playbook说明的代码段。 -v:显示工具版本号。【示例1】安装redhat-lsb并查看服务器系统版本号。步骤1：安装redhat-lsb123456789101112131415161718192021222324[root@Ansible ~]# ansible all -m yum -a &quot;name=redhat-lsb state=present&quot;36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;redhat-lsb-4.1-27.el7.centos.1.x86_64 providing redhat-lsb is already installed&quot; ]&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;redhat-lsb-4.1-27.el7.centos.1.x86_64 providing redhat-lsb is already installed&quot; ]&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;redhat-lsb-4.1-27.el7.centos.1.x86_64 providing redhat-lsb is already installed&quot; ]步骤2：查看系统版本号123456789101112131415161718192021[root@Ansible ~]# ansible all -m command -a &quot;lsb_release -a&quot;36.103.245.138 | SUCCESS | rc=0 &gt;&gt;LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611Codename: Core36.103.245.70 | SUCCESS | rc=0 &gt;&gt;LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611Codename: Core36.103.245.135 | SUCCESS | rc=0 &gt;&gt;LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611Codename: Core【示例2】为所有服务器安装ntp服务,并设置为开机启动步骤1：安装ntp服务12345678910111213141516171819202122232425[root@Ansible ~]# ansible all -s -m yum -a &quot;name=ntp state=present&quot;36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;ntp-4.2.6p5-28.el7.centos.x86_64 providing ntp is already installed&quot; ]&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;ntp-4.2.6p5-28.el7.centos.x86_64 providing ntp is already installed&quot; ]&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;ntp-4.2.6p5-28.el7.centos.x86_64 providing ntp is already installed&quot; ]&#125;步骤2：启动ntp服务,并设置为开机启动12345678910111213141516171819202122232425262728[root@Ansible ~]# ansible all -m service -a &quot;name=ntpd state=started enabled=yes&quot;36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;enabled&quot;: true, &quot;name&quot;: &quot;ntpd&quot;, &quot;state&quot;: &quot;started&quot;, &quot;status&quot;: &#123; ...... &#125;&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;enabled&quot;: true, &quot;name&quot;: &quot;ntpd&quot;, &quot;state&quot;: &quot;started&quot;, &quot;status&quot;: &#123; ...... &#125;&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;enabled&quot;: true, &quot;name&quot;: &quot;ntpd&quot;, &quot;state&quot;: &quot;started&quot;, &quot;status&quot;: &#123; ...... &#125;&#125;","categories":[{"name":"Ansable","slug":"Ansable","permalink":"http://blog.wubolive.com/categories/Ansable/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://blog.wubolive.com/tags/Ansible/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"}]},{"title":"Ansible常用模块收录册","slug":"ansible-mode","date":"2018-11-30T06:11:28.000Z","updated":"2018-11-30T13:43:17.726Z","comments":true,"path":"2018/11/30/ansible-mode/","link":"","permalink":"http://blog.wubolive.com/2018/11/30/ansible-mode/","excerpt":"","text":"Ansible常用模块，长期更新…yum模块yum：RedHat/CentOS包管理工具12345678910111213常用选项：– config_file：yum的配置文件 （optional） – disable_gpg_check：关闭gpg_check （optional） – disablerepo：不启用某个源 （optional） – enablerepo：启用某个源（optional） – name：要进行操作的软件包的名字，默认最新的程序包，指明要安装的程序包，可以带上版本号，也可以传递一个url或者一个本地的rpm包的路径 – state：状态（present，absent，latest），表示是安装还卸载 present:默认的，表示为安装 lastest: 安装为最新的版本 absent：表示删除 示例：[root@Ansible ~]# ansible all -m yum -a &apos;name=httpd state=latest&apos;","categories":[{"name":"Ansable","slug":"Ansable","permalink":"http://blog.wubolive.com/categories/Ansable/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://blog.wubolive.com/tags/Ansible/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"}]},{"title":"Ansible安装与配置","slug":"ansible-install","date":"2018-11-30T06:10:28.000Z","updated":"2018-11-30T13:43:11.878Z","comments":true,"path":"2018/11/30/ansible-install/","link":"","permalink":"http://blog.wubolive.com/2018/11/30/ansible-install/","excerpt":"","text":"本文讲述如何安装Ansible环境，了解Ansible基本配置、运行测试安装AnsibleCentOS 7系统安装Ansible(yum方式)1234#安装ansible yum源rpm -Uvh http://mirrors.zju.edu.cn/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm#yum安装ansibleyum -y install ansible配置Ansible主配置文件常用配置12345678910111213141516171819[root@CentOS ~]# vim /etc/ansible/ansible.cfg[defaults]# 存放主机列表文件inventory = /etc/ansible/hosts# 指向Ansible模块目录library = /usr/share/my_modules/# 配置Ansible最大运行进程forks = 5# 设置默认执行命令的用户sudo_user = root# 配置管理节点的管理端口remote_port = 22# 配置是否检查SSH主机密钥host_key_checking = False# SSH超时时间timeout = 60# 指定存放Ansible日志的文件log_path = /var/log/ansible.log业务环境角色主机名IP地址组名控制主机ansable36.103.245.156—被管理节点master36.103.245.135k8server被管理节点node136.103.245.138k8server被管理节点node236.103.245.70k8server配置Linux主机ssh无密钥访问12345678910111213141516171819202122232425#生成密钥[root@Ansible ~]# ssh-keygen#将公钥分发到被管理节点[root@Ansible ~]# ssh-copy-id -i .ssh/id_rsa.pub root@36.103.245.135/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;.ssh/id_rsa.pub&quot;The authenticity of host &apos;36.103.245.135 (36.103.245.135)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:ZTtQLCTg21cYLQ5iJa5LkC51xN6lKGxVyLRAxjXPUOw.ECDSA key fingerprint is MD5:6d:5b:e9:d9:bd:12:64:06:c5:cc:a2:07:a6:99:96:3d.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@36.103.245.135&apos;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &apos;root@36.103.245.135&apos;&quot;and check to make sure that only the key(s) you wanted were added.## 对node1及node2作相同的配置#测试是否免密登入[root@Ansible ~]# ssh 36.103.245.135Last failed login: Wed Sep 5 10:06:49 CST 2018 from 118.24.129.24 on ssh:nottyThere were 27 failed login attempts since the last successful login.Last login: Wed Sep 5 09:50:20 2018 from 36.103.245.156Ansible 小测试首先可以查看一下ansible的软件版本信息1234567[root@Ansible ~]# ansible --versionansible 2.6.3 config file = /etc/ansible/ansible.cfg configured module search path = [u&apos;/usr/share/my_modules&apos;] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Nov 6 2016, 00:28:07) [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)]主机连通性测试使用Ansible需要先定于主机与组的配置，默认文件在/etc/ansible/hosts12345678910111213141516[root@Ansible ~]# cat /etc/ansible/hosts# 定义主机#master.wubolive.com#node1.wubolive.com#node2.wubolive.com36.103.245.13536.103.245.13836.103.245.70# 定义组(组名需要用[]括起来)[k8server]#master.wubolive.com#node1.wubolive.com#node2.wubolive.com36.103.245.13536.103.245.13836.103.245.70定义完成后使用ping模块对单主机进行ping操作12345[root@Ansible ~]# ansible 36.103.245.135 -m ping36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;对k8server组进行ping操作12345678910111213[root@Ansible ~]# ansible k8server -m ping 36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;在被管理节点批量执行命令在用户目录创建一个资源清单文件inventory.cfg12345[root@Ansible ~]# cat inventory.cfg [k8server]36.103.245.13536.103.245.13836.103.245.70用ansible的shell模块对k8server组各服务器显示‘hello ansible！’123456789[root@Ansible ~]# ansible k8server -m shell -a &apos;/bin/echo hello ansible!&apos; -i inventory.cfg 36.103.245.70 | SUCCESS | rc=0 &gt;&gt;hello ansible!36.103.245.138 | SUCCESS | rc=0 &gt;&gt;hello ansible!36.103.245.135 | SUCCESS | rc=0 &gt;&gt;hello ansible!用command模块也可以执行相同操作123456789[root@Ansible ~]# ansible k8server -m command -a &apos;/bin/echo hello ansible!&apos; -i inventory.cfg 36.103.245.138 | SUCCESS | rc=0 &gt;&gt;hello ansible!36.103.245.70 | SUCCESS | rc=0 &gt;&gt;hello ansible!36.103.245.135 | SUCCESS | rc=0 &gt;&gt;hello ansible!获取帮助信息12345678910# 查看ansible命令帮助使用-h选项[root@Ansible ~]# ansible -h# 用ansible-doc列出ansible系统支持的模块[root@Ansible ~]# ansible-doc -l# 用ansible-doc加模块名称，可以显示该模块的描述和使用示例[root@Ansible ~]# ansible-doc yum# ansible-doc -s选项可以列出模块的动作[root@Ansible ~]# ansible-doc -s yum另外，在Ansible调试脚本时，可以使用-v或者-vvv显示详细的输出结果123[root@Ansible ~]# ansible k8server -i inventory.cfg -m ping -v# 或[root@Ansible ~]# ansible k8server -i inventory.cfg -m ping -vvv","categories":[{"name":"Ansable","slug":"Ansable","permalink":"http://blog.wubolive.com/categories/Ansable/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://blog.wubolive.com/tags/Ansible/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"}]}]}