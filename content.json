{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://blog.wubolive.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-11-30T09:49:48.963Z","updated":"2018-11-30T09:49:48.950Z","comments":false,"path":"/404.html","permalink":"http://blog.wubolive.com//404.html","excerpt":"","text":""},{"title":"关于","date":"2018-11-30T09:49:49.464Z","updated":"2018-11-30T09:49:48.955Z","comments":false,"path":"about/index.html","permalink":"http://blog.wubolive.com/about/index.html","excerpt":"","text":"一个来自南方的北漂少年 · 阿波Linux | 云计算 | DevOps | 数据库 | Python | 大数据本人比较喜欢使用Markdown语法写文章，所以之前用Hexo驱动在github.com中搭建了一个博客http://blog.wubolive.com，但因为管理不灵活而将博客迁移到Leanote 新浪微博：@那繁华之处"},{"title":"分类","date":"2018-11-30T09:49:49.489Z","updated":"2018-11-30T09:49:48.960Z","comments":false,"path":"categories/index.html","permalink":"http://blog.wubolive.com/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2018-11-30T09:49:48.958Z","updated":"2018-11-30T09:49:48.958Z","comments":false,"path":"books/index.html","permalink":"http://blog.wubolive.com/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-11-30T09:49:48.963Z","updated":"2018-11-30T09:49:48.962Z","comments":true,"path":"links/index.html","permalink":"http://blog.wubolive.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-11-30T09:49:48.964Z","updated":"2018-11-30T09:49:48.964Z","comments":false,"path":"repository/index.html","permalink":"http://blog.wubolive.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-11-30T09:49:49.521Z","updated":"2018-11-30T09:49:48.965Z","comments":false,"path":"tags/index.html","permalink":"http://blog.wubolive.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"CentOS7编译安装python3","slug":"centos7-install-python3","date":"2019-03-26T06:36:28.000Z","updated":"2019-05-13T10:12:01.815Z","comments":true,"path":"2019/03/26/centos7-install-python3/","link":"","permalink":"http://blog.wubolive.com/2019/03/26/centos7-install-python3/","excerpt":"","text":"安装编译相关软件123yum -y groupinstall &quot;Development tools&quot;yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-develyum install libffi-devel -y下载解压安装包12wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tar.xztar xvJf Python-3.7.0.tar.xz编译安装123cd Python-3.7.0/./configure --prefix=/usr/local/python3make &amp;&amp; make install备份并添加python3命令12345cd /usr/bin/mv python python.bakll python*ln -s /usr/local/python3/bin/python3.7 /usr/bin/pythonln -s /usr/local/python3/bin/pip3.7 /usr/bin/pip验证1234[root@CentOS7 ~]# python -VPython 3.7.0[root@CentOS7 ~]# pip -Vpip 10.0.1 from /usr/local/python3/lib/python3.7/site-packages/pip (python 3.7)解决yum和sqlite-devel依赖修改以下两个配置文件的解释器改成python2.71234vim /usr/bin/yumvim /usr/libexec/urlgrabber-ext-downpython2.7#!/usr/bin/python2.7","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.wubolive.com/tags/Python/"}]},{"title":"CentOS7编译安装python3","slug":"python3-install","date":"2019-03-17T07:00:28.000Z","updated":"2019-05-14T01:47:01.666Z","comments":true,"path":"2019/03/17/python3-install/","link":"","permalink":"http://blog.wubolive.com/2019/03/17/python3-install/","excerpt":"","text":"安装编译相关软件123yum -y groupinstall &quot;Development tools&quot;yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-develyum install libffi-devel -y下载解压安装包12wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tar.xztar xvJf Python-3.7.0.tar.xz编译安装123cd Python-3.7.0/./configure --prefix=/usr/local/python3make &amp;&amp; make install备份并添加python3命令12345cd /usr/bin/mv python python.bakll python*ln -s /usr/local/python3/bin/python3.7 /usr/bin/pythonln -s /usr/local/python3/bin/pip3.7 /usr/bin/pip验证1234[root@CentOS7 ~]# python -VPython 3.7.0[root@CentOS7 ~]# pip -Vpip 10.0.1 from /usr/local/python3/lib/python3.7/site-packages/pip (python 3.7)解决yum和sqlite-devel依赖修改以下两个配置文件的解释器改成python2.71234vim /usr/bin/yumvim /usr/libexec/urlgrabber-ext-downpython2.7#!/usr/bin/python2.7","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"python","slug":"python","permalink":"http://blog.wubolive.com/tags/python/"}]},{"title":"源码安装Kubernetes1.13","slug":"kubernetes1.13","date":"2018-12-25T01:17:28.000Z","updated":"2018-12-25T01:55:54.455Z","comments":true,"path":"2018/12/25/kubernetes1.13/","link":"","permalink":"http://blog.wubolive.com/2018/12/25/kubernetes1.13/","excerpt":"","text":"环境说明：角色IP组件k8s-master172.17.0.81kube-apiserver,kube-controller-manage,kube-scheduler,etcdk8s-node1172.17.0.82kubelet,kube-proxy,docker,flannel,etcdk8s-node2172.17.0.83kubelet,kube-proxy,docker,flannel,etcd12345[root@k8s-master ~]# cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core)[root@k8s-master ~]# uname -aLinux k8s-master 4.19.11-1.el7.elrepo.x86_64 #1 SMP Wed Dec 19 14:24:42 EST 2018 x86_64 x86_64 x86_64 GNU/Linux环境准备系统配置配置hostname并添加至hosts123456789101112[root@k8s-master ~]# hostnamectl set-hostname [k8s-master|k8s-node1|k8s-node2][root@k8s-master ~]# cat &gt;&gt; /etc/hosts &lt;&lt; EOF172.17.0.81 k8s-master172.17.0.82 k8s-node1172.17.0.83 k8s-node2EOF# 配置免密[root@k8s-master ~]# ssh-keygen -t rsa[root@k8s-master ~]# ssh-copy-id k8s-node1[root@k8s-master ~]# ssh-copy-id k8s-node2配置时间同步123456[root@k8s-master ~]# yum -y install ntp[root@k8s-master ~]# systemctl enable ntpd[root@k8s-master ~]# systemctl start ntpd[root@k8s-master ~]# ntpdate -u cn.pool.ntp.org[root@k8s-master ~]# hwclock --systohc[root@k8s-master ~]# timedatectl set-timezone Asia/Shangha关闭swap分区123[root@k8s-master ~]# swapoff -a &amp;&amp; sysctl -w vm.swappiness=0[root@k8s-master ~]# vim /etc/fstab#UUID=7bff6243-324c-4587-b550-55dc34018ebf swap swap defaults关闭防火墙及Slinux1234[root@k8s-master ~]# systemctl stop firewalld &amp;&amp; systemctl disable firewalld[root@k8s-master ~]# setenforce 0[root@k8s-master ~]# vim /etc/selinux/config[root@k8s-master ~]# SELINUX=disabled配置yum源12345[root@k8s-master ~]# curl -o /etc/yum.repos.d/CentOS-Base.repo[root@k8s-master ~]# http://mirrors.aliyun.com/repo/Centos-7.repo[root@k8s-master ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo[root@k8s-master ~]# yum makecache[root@k8s-master ~]# yum install wget vim lsof net-tools lrzsz -y升级并优化内核123456[root@k8s-master ~]# yum update [root@k8s-master ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org[root@k8s-master ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm[root@k8s-master ~]# yum --enablerepo=elrepo-kernel install kernel-ml -y&amp;&amp;[root@k8s-master ~]# sed -i s/saved/0/g /etc/default/grub&amp;&amp;[root@k8s-master ~]# grub2-mkconfig -o /boot/grub2/grub.cfg &amp;&amp; reboot优化内核参数1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master ~]# cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF- soft nofile 1024000- hard nofile 1024000- soft noroc 1024000- hard nproc 1024000EOF[root@k8s-master ~]# cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv4.tcp_tw_recycle = 0net.ipv4.ip_local_port_range = 10000 61000net.ipv4.tcp_syncookies = 1net.ipv4.tcp_fin_timeout = 30net.ipv4.ip_forward = 1net.core.netdev_max_backlog = 2000net.ipv4.tcp_mem = 131072 262144 524288net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 2048net.ipv4.tcp_low_latency = 0net.core.rmem_default = 256960net.core.rmem_max = 513920net.core.wmem_default = 256960net.core.wmem_max = 513920net.core.somaxconn = 2048net.core.optmem_max = 81920net.ipv4.tcp_mem = 131072 262144 524288net.ipv4.tcp_rmem = 8760 256960 4088000net.ipv4.tcp_wmem = 8760 256960 4088000net.ipv4.tcp_keepalive_time = 1800net.ipv4.tcp_sack = 1net.ipv4.tcp_fack = 1net.ipv4.tcp_timestamps = 1net.ipv4.tcp_syn_retries = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-arptables = 1EOF[root@k8s-master ~]# sysctl -p以上配置及优化内容需要在集群三台中都要配置Node节点安装Docker12345678# 从国内源下载并安装docker-ce[root@k8s-node1 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2[root@k8s-node1 ~]# yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo[root@k8s-node1 ~]# yum install docker-ce -y# 配置docker国内镜像源并启动[root@k8s-node1 ~]# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://bc437cce.m.daocloud.io[root@k8s-node1 ~]# systemctl start docker[root@k8s-node1 ~]# systemctl enable docker部署Etcd集群创建安装目录12345678910# 三台主机都可先创建好[root@k8s-master ~]# mkdir /home/etcd/&#123;bin,cfg,ssl&#125; -p[root@k8s-master ~]# mkdir /home/kubernetes/&#123;bin,cfg,ssl&#125; -p# 配置环境变量[root@k8s-master ~]# tail /etc/profileK8S_HOME=/home/kubernetesETCD_HOME=/home/etcdexport PATH=$PATH:$K8S_HOME/bin:$ETCD_HOME/bin[root@k8s-master ~]# source /etc/profile安装及配置CFSSL1234567[root@k8s-master ~]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64[root@k8s-master ~]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64[root@k8s-master ~]# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64[root@k8s-master ~]# chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64[root@k8s-master ~]# mv cfssl_linux-amd64 /usr/local/bin/cfssl[root@k8s-master ~]# mv cfssljson_linux-amd64 /usr/local/bin/cfssljson[root@k8s-master ~]# mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo创建Etcd认证证书123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@k8s-master ~]# mkdir /root/etcd_ssl &amp;&amp; cd /root/etcd_ssl#创建Etcd CA配置文件[root@k8s-master ~]# cat &lt;&lt; EOF | tee ca-config.json&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"www\": &#123; \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ] &#125; &#125; &#125;&#125;EOF#创建Etcd CA 配置文件[root@k8s-master ~]# cat &lt;&lt; EOF | tee ca-csr.json&#123; \"CN\": \"etcd CA\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"Shenzhen\", \"ST\": \"Shenzhen\" &#125; ]&#125;EOF#创建Etcd Server证书[root@k8s-master ~]# cat &lt;&lt; EOF | tee server-csr.json&#123; \"CN\": \"etcd\", \"hosts\": [ \"172.17.0.81\", \"172.17.0.82\", \"172.17.0.83\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"Shenzhen\", \"ST\": \"Shenzhen\" &#125; ]&#125;EOF生成Etcd CA证书和私钥12[root@k8s-master ~]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -[root@k8s-master ~]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server将生成的证书文件复制到配置文件ssl目录12345[root@k8s-master ~]# cp *.pem /home/etcd/ssl/# 将证书复制到node节点[root@k8s-master ~]# scp *.pem k8s-node1:/home/etcd/ssl/[root@k8s-master ~]# scp *.pem k8s-node2:/home/etcd/ssl/安装Etcd二进制包下载地址：https://github.com/etcd-io/etcd/releases/12345678[root@k8s-master ~]# wget https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz[root@k8s-master ~]# tar zxf etcd-v3.3.10-linux-amd64.tar.gz [root@k8s-master ~]# cd etcd-v3.3.10-linux-amd64[root@k8s-master etcd-v3.3.10-linux-amd64]# cp etcd etcdctl /home/etcd/bin/# 将启动文件复制到node节点[root@k8s-master etcd-v3.3.10-linux-amd64]# scp etcd etcdctl k8s-node1:/home/etcd/bin/[root@k8s-master etcd-v3.3.10-linux-amd64]# scp etcd etcdctl k8s-node2:/home/etcd/bin/创建Etcd主配置文件12345678910111213[root@k8s-master ~]# vim /home/etcd/cfg/etcd#[Member]ETCD_NAME=\"etcd01\"ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"ETCD_LISTEN_PEER_URLS=\"https://172.17.0.81:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://172.17.0.81:2379\"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.17.0.81:2380\"ETCD_ADVERTISE_CLIENT_URLS=\"https://172.17.0.81:2379\"ETCD_INITIAL_CLUSTER=\"etcd01=https://172.17.0.81:2380,etcd02=https://172.17.0.82:2380,etcd03=https://172.17.0.83:2380\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_INITIAL_CLUSTER_STATE=\"new\"创建Etcd的systemd unit 文件12345678910111213141516171819202122232425262728293031[root@k8s-master ~]# vim /usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/home/etcd/cfg/etcdExecStart=/home/etcd/bin/etcd \\--name=$&#123;ETCD_NAME&#125; \\--data-dir=$&#123;ETCD_DATA_DIR&#125; \\--listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \\--listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \\--advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \\--initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \\--initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \\--initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \\--initial-cluster-state=new \\--cert-file=/home/etcd/ssl/server.pem \\--key-file=/home/etcd/ssl/server-key.pem \\--peer-cert-file=/home/etcd/ssl/server.pem \\--peer-key-file=/home/etcd/ssl/server-key.pem \\--trusted-ca-file=/home/etcd/ssl/ca.pem \\--peer-trusted-ca-file=/home/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target将配置文件发送到node节点1234567# 复制etcd配置文件到node节点[root@k8s-master ~]# scp /home/etcd/cfg/etcd k8s-node1:/home/etcd/cfg/[root@k8s-master ~]# scp /home/etcd/cfg/etcd k8s-node2:/home/etcd/cfg/# 复制systemd unit文件至node节点[root@k8s-master ~]# scp /usr/lib/systemd/system/etcd.service k8s-node1:/usr/lib/systemd/system/[root@k8s-master ~]# scp /usr/lib/systemd/system/etcd.service k8s-node2:/usr/lib/systemd/system/启动Etcd集群12345[root@k8s-master ~]# systemctl daemon-reload[root@k8s-master ~]# systemctl start etcd[root@k8s-master ~]# systemctl enable etcd# 注：以上步骤在规划的2个node节点操作是一样的，除了etcd配置文件中的服务器IP和ETCD_NAME验证Etcd集群123456789101112etcdctl --ca-file=/home/etcd/ssl/ca.pem \\--cert-file=/home/etcd/ssl/server.pem \\--key-file=/home/etcd/ssl/server-key.pem \\--endpoints=\"https://172.17.0.81:2379,\\https://172.17.0.82:2379,\\https://172.17.0.83:2379\" cluster-health# 输出以下结果说明创建成功member 2a795d9614d1f9de is healthy: got healthy result from https://172.17.0.82:2379member baca0e6b543a685b is healthy: got healthy result from https://172.17.0.83:2379member e7084551225d95f5 is healthy: got healthy result from https://172.17.0.81:2379cluster is healthy部署Flannel网络向Etcd写入集群Pod网段信息该步骤只需在第一次部署Flannel 网络时执行，后续在其他节点上部署Flanneld 时无需再写入该信息123456789[root@k8s-master ~]# etcdctl --ca-file=/home/etcd/ssl/ca.pem \\--cert-file=/home/etcd/ssl/server.pem \\--key-file=/home/etcd/ssl/server-key.pem \\--endpoints=\"https://172.17.0.81:2379,\\https://172.17.0.82:2379,https://172.17.0.83:2379\" \\set /coreos.com/network/config '&#123; \"Network\": \"172.18.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;'# 输出如下信息说明创建成功&#123; \"Network\": \"172.18.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;flanneld 当前版本 (v0.10.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；写入的 Pod 网段 ${CLUSTER_CIDR} 必须是 /16 段地址，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致；安装及配置Flanneld以下步骤在node节点中执行1234# 下载二进制包[root@k8s-node1 ~]# wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz[root@k8s-node1 ~]# tar zxf flannel-v0.10.0-linux-amd64.tar.gz [root@k8s-node1 ~]# mv flanneld mk-docker-opts.sh /home/kubernetes/bin/配置Flanneld123[root@k8s-node1 ~]# cat &gt; /home/kubernetes/cfg/flanneld &lt;&lt; EOFFLANNEL_OPTIONS=\"--etcd-endpoints=https://172.17.0.81:2379,https://172.17.0.82:2379,https://172.17.0.83:2379 -etcd-cafile=/home/etcd/ssl/ca.pem -etcd-certfile=/home/etcd/ssl/server.pem -etcd-keyfile=/home/etcd/ssl/server-key.pem\"EOF创建flanneld的systemd unit文件12345678910111213141516[root@k8s-node1 ~]# cat &gt; /usr/lib/systemd/system/flanneld.service &lt;&lt; EOF[Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]Type=notifyEnvironmentFile=/home/kubernetes/cfg/flanneldExecStart=/home/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONSExecStartPost=/home/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure[Install]WantedBy=multi-user.targetEOFmk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入 /run/flannel/docker 文件，后续 docker 启动时 使用这个文件中的环境变量配置 docker0 网桥；flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口，如上面的 eth0 接口;flanneld 运行时需要 root 权限；配置Docker启动指定子网段12345678910111213141516171819202122232425262728[root@k8s-node1 ~]# cat &gt; /usr/lib/systemd/system/docker.service &lt;&lt; EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comBindsTo=containerd.serviceAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notifyEnvironmentFile=/run/flannel/subnet.envExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONSExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=process[Install]WantedBy=multi-user.targetEOF将启动文件及配置文件复制到node2上1234[root@k8s-node1 ~]# scp /home/kubernetes/cfg/flanneld k8s-node2:/home/kubernetes/cfg/[root@k8s-node1 ~]# scp /home/kubernetes/bin/&#123;flanneld,mk-docker-opts.sh&#125; k8s-node2:/home/kubernetes/bin/[root@k8s-node1 ~]# scp /usr/lib/systemd/system/flanneld.service k8s-node2:/usr/lib/systemd/system/flanneld.service[root@k8s-node1 ~]# scp /usr/lib/systemd/system/docker.service k8s-node2:/usr/lib/systemd/system/docker.service启动flanneld并重启docker1234[root@k8s-node1 ~]# systemctl daemon-reload[root@k8s-node1 ~]# systemctl start flanneld[root@k8s-node1 ~]# systemctl enable flanneld[root@k8s-node1 ~]# systemctl restart docker验证Flanneld是否生效1234567891011121314151617181920212223[root@k8s-node1 ~]# ip add1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:c3:5f:7e brd ff:ff:ff:ff:ff:ff inet 172.17.0.82/23 brd 172.17.1.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:48:39:11:70 brd ff:ff:ff:ff:ff:ff inet 172.18.21.1/24 brd 172.18.21.255 scope global docker0 valid_lft forever preferred_lft forever4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 6e:a1:64:5c:ec:a3 brd ff:ff:ff:ff:ff:ff inet 172.18.21.0/32 scope global flannel.1 valid_lft forever preferred_lft forever # 从node1上ping测试node2的docker0网桥IP是否能通信[root@k8s-node1 ~]# ping 172.18.43.1PING 172.18.43.1 (172.18.43.1) 56(84) bytes of data.64 bytes from 172.18.43.1: icmp_seq=1 ttl=64 time=0.496 ms64 bytes from 172.18.43.1: icmp_seq=2 ttl=64 time=0.522 ms部署Master节点Master组件简介kube-apiserver: Kubernetes API，集群的统一入口，各组件协调者，以RESTful API提供接口服务，所有对象资源的增删改查和监听操作都交给APIServer处理后再提交给Etcd存储。kube-controller-manager: 处理集群中常规后台任务，一个资源对应一个控制器，而ControllerManager就是负责管理这些控制器的。kube-scheduler: 根据调度算法为新创建的Pod选择一个Node节点，可以任意部署,可以部署在同一个节点上,也可以部署在不同的节点上kube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。生成kubernetes证书创建kubernetes CA证书123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master ~]# mkdir /root/kubernetes_ssl &amp;&amp; cd /root/kubernetes_ssl[root@k8s-master kubernetes_ssl]# cat &lt;&lt; EOF | tee ca-config.json&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ] &#125; &#125; &#125;&#125;EOF[root@k8s-master kubernetes_ssl]# cat &lt;&lt; EOF | tee ca-csr.json&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"Shenzhen\", \"ST\": \"Shenzhen\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF[root@k8s-master kubernetes_ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -生成API Sercer证书1234567891011121314151617181920212223242526272829303132[root@k8s-master kubernetes_ssl]# cat &lt;&lt; EOF | tee server-csr.json&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"10.0.0.1\", \"127.0.0.1\", \"172.17.0.81\", \"172.17.0.82\", \"172.17.0.83\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"Shenzhen\", \"ST\": \"Shenzhen\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF[root@k8s-master kubernetes_ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server创建Kubernetes Proxy证书123456789101112131415161718192021[root@k8s-master kubernetes_ssl]# cat &lt;&lt; EOF | tee kube-proxy-csr.json&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"Shenzhen\", \"ST\": \"Shenzhen\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF[root@k8s-master kubernetes_ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy完成如上操作将会生成以下证书文件:12345[root@k8s-master kubernetes_ssl]# ls *.pemca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem# 将证书拷贝到kubernetes工作目录[root@k8s-master kubernetes_ssl]# cp *.pem /home/kubernetes/ssl/部署 kube-apiserver 组件到GitHub上下载二进制包https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.13.md1234[root@k8s-master ~]# wget https://dl.k8s.io/v1.13.1/kubernetes-server-linux-amd64.tar.gz[root@k8s-master ~]# tar zxf kubernetes-server-linux-amd64.tar.gz [root@k8s-master ~]# cd kubernetes/server/bin/[root@k8s-master bin]# cp kube-apiserver kube-scheduler kube-controller-manager kubectl /home/kubernetes/bin/创建 TLS Bootstrapping Token12345[root@k8s-master ~]# head -c 16 /dev/urandom | od -An -t x | tr -d ' '1d02aaf678bee18f06a3bf1804715036[root@k8s-master ~]# vim /home/kubernetes/cfg/token.csv1d02aaf678bee18f06a3bf1804715036,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"创建apiserver配置文件123456789101112131415161718192021[root@k8s-master ~]# vim /home/kubernetes/cfg/kube-apiserverKUBE_APISERVER_OPTS=\"--logtostderr=true \\--v=4 \\--etcd-servers=https://172.17.0.81:2379,https://172.17.0.82:2379,https://172.17.0.83:2379 \\--bind-address=172.17.0.81 \\--secure-port=6443 \\--advertise-address=172.17.0.81 \\--allow-privileged=true \\--service-cluster-ip-range=10.0.0.0/24 \\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC,Node \\--enable-bootstrap-token-auth \\--token-auth-file=/home/kubernetes/cfg/token.csv \\--service-node-port-range=30000-50000 \\--tls-cert-file=/home/kubernetes/ssl/server.pem \\--tls-private-key-file=/home/kubernetes/ssl/server-key.pem \\--client-ca-file=/home/kubernetes/ssl/ca.pem \\--service-account-key-file=/home/kubernetes/ssl/ca-key.pem \\--etcd-cafile=/home/etcd/ssl/ca.pem \\--etcd-certfile=/home/etcd/ssl/server.pem \\--etcd-keyfile=/home/etcd/ssl/server-key.pem\"参数说明：--logtostderr 启用日志---v 日志等级--etcd-servers etcd集群地址--bind-address 监听地址--secure-port https安全端口--advertise-address 集群通告地址--allow-privileged 启用授权--service-cluster-ip-range Service虚拟IP地址段--enable-admission-plugins 准入控制模块--authorization-mode 认证授权，启用RBAC授权和节点自管理--enable-bootstrap-token-auth 启用TLS bootstrap功能，后面会讲到--token-auth-file token文件--service-node-port-range Service Node类型默认分配端口范围创建 kube-apiserver systemd unit 文件123456789101112[root@k8s-master ~]# vim /usr/lib/systemd/system/kube-apiserver.service [Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/home/kubernetes/cfg/kube-apiserverExecStart=/home/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target启动服务1234[root@k8s-master ~]# systemctl daemon-reload[root@k8s-master ~]# systemctl start kube-apiserver[root@k8s-master ~]# systemctl enable kube-apiserver[root@k8s-master ~]# systemctl status kube-apiserver部署kube-scheduler创建kube-scheduler配置文件12[root@k8s-master ~]# vim /home/kubernetes/cfg/kube-scheduler KUBE_SCHEDULER_OPTS=\"--logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect\"–address：在 127.0.0.1:10251 端口接收 http /metrics 请求；kube-scheduler 目前还不支持接收 https 请求；–kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；–leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；创建kube-scheduler systemd unit 文件123456789101112[root@k8s-master ~]# vim /usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/home/kubernetes/cfg/kube-schedulerExecStart=/home/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target启动kuber-scheduler服务1234[root@k8s-master ~]# systemctl daemon-reload[root@k8s-master ~]# systemctl start kube-scheduler[root@k8s-master ~]# systemctl enable kube-scheduler[root@k8s-master ~]# systemctl status kube-scheduler部署kube-controller-manager创建kube-controller-manager配置文件123456789101112[root@k8s-master ~]# vim /home/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=\"--logtostderr=true \\--v=4 \\--master=127.0.0.1:8080 \\--leader-elect=true \\--address=127.0.0.1 \\--service-cluster-ip-range=10.0.0.0/24 \\--cluster-name=kubernetes \\--cluster-signing-cert-file=/home/kubernetes/ssl/ca.pem \\--cluster-signing-key-file=/home/kubernetes/ssl/ca-key.pem \\--root-ca-file=/home/kubernetes/ssl/ca.pem \\--service-account-private-key-file=/home/kubernetes/ssl/ca-key.pem\"创建kube-controller-manager systemd unit 文件123456789101112[root@k8s-master ~]# vim /usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/home/kubernetes/cfg/kube-controller-managerExecStart=/home/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target启动controller-manager服务1234[root@k8s-master ~]# systemctl daemon-reload[root@k8s-master ~]# systemctl start kube-controller-manager[root@k8s-master ~]# systemctl enable kube-controller-manager[root@k8s-master ~]# systemctl status kube-controller-manager查看master集群状态1234567[root@k8s-master ~]# kubectl get cs,nodesNAME STATUS MESSAGE ERRORcomponentstatus/scheduler Healthy ok componentstatus/controller-manager Healthy ok componentstatus/etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; componentstatus/etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;部署Node 节点Node组件简介kubelet: kubelet是Master在Node节点上的Agent，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。kubelet将每个Pod转换成一组容器。kube-proxy: 在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作。docker： 容器引擎etcd: 分布式键值存储系统。用于保存集群状态数据，比如Pod、Service等对象信息。将kubelet二进制文件拷贝到node节点上12[root@k8s-master ~]# scp kubernetes/server/bin/&#123;kubelet,kube-proxy&#125; k8s-node1:/home/kubernetes/bin/[root@k8s-master ~]# scp kubernetes/server/bin/&#123;kubelet,kube-proxy&#125; k8s-node2:/home/kubernetes/bin/部署kubelet组件创建 kubelet bootstrap kubeconfig 文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@k8s-master ~]# cd kubernetes_ssl/[root@k8s-master kubernetes_ssl]# BOOTSTRAP_TOKEN=1d02aaf678bee18f06a3bf1804715036[root@k8s-master kubernetes_ssl]# KUBE_APISERVER=\"172.17.0.81:6443\"# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=./ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig#----------------------# 创建kube-proxy kubeconfig文件kubectl config set-cluster kubernetes \\ --certificate-authority=./ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \\ --client-certificate=./kube-proxy.pem \\ --client-key=./kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig[root@k8s-master kubernetes_ssl]# ls *.kubeconfigbootstrap.kubeconfig kube-proxy.kubeconfig将bootstrap kubeconfig kube-proxy.kubeconfig 文件拷贝到所有 nodes节点1234[root@k8s-master kubernetes_ssl]# scp *.pem k8s-node1:/home/kubernetes/ssl/[root@k8s-master kubernetes_ssl]# scp *.pem k8s-node2:/home/kubernetes/ssl/[root@k8s-master kubernetes_ssl]# scp *.kubeconfig k8s-node1:/home/kubernetes/cfg/[root@k8s-master kubernetes_ssl]# scp *.kubeconfig k8s-node2:/home/kubernetes/cfg/创建 kubelet 参数配置模板文件：12345678910111213[root@k8s-node1 ~]# vim /home/kubernetes/cfg/kubelet.configkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 172.17.0.82port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [\"10.0.0.2\"]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true创建kubelet配置文件123456789[root@k8s-node1 ~]# vim /home/kubernetes/cfg/kubeletKUBELET_OPTS=\"--logtostderr=true \\--v=4 \\--hostname-override=172.17.0.82 \\--kubeconfig=/home/kubernetes/cfg/kubelet.kubeconfig \\--bootstrap-kubeconfig=/home/kubernetes/cfg/bootstrap.kubeconfig \\--config=/home/kubernetes/cfg/kubelet.config \\--cert-dir=/home/kubernetes/ssl \\--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0\"参数说明：--hostname-override 在集群中显示的主机名--kubeconfig 指定kubeconfig文件位置，会自动生成--bootstrap-kubeconfig 指定刚才生成的bootstrap.kubeconfig文件--cert-dir 颁发证书存放位置--cluster-dns 集群DNS IP，先配置上，后面会讲到--cluster-domain DNS域--fail-swap-on=false 禁止使用swap--pod-infra-container-image 管理Pod网络的镜像创建kubelet systemd unit 文件1234567891011121314[root@k8s-node1 ~]# vim /usr/lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=/home/kubernetes/cfg/kubeletExecStart=/home/kubernetes/bin/kubelet $KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.target将kubelet-bootstrap用户绑定到系统集群角色123456[root@k8s-master ~]# kubectl create clusterrolebinding kubelet-bootstrap \\--clusterrole=system:node-bootstrapper \\--user=kubelet-bootstrap# 输出内容clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created启动kubelet服务1234[root@k8s-node1 ~]# systemctl daemon-reload [root@k8s-node1 ~]# systemctl start kubelet [root@k8s-node1 ~]# systemctl enable kubelet[root@k8s-node1 ~]# systemctl status kubeletapprove kubelet CSR 请求可以手动或自动 approve CSR 请求。推荐使用自动的方式，因为从 v1.8 版本开始，可以自动轮转approve csr 后生成的证书。12345678910111213141516# 查看 CSR 列表：[root@k8s-master ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-W9gLNYYT3Nk4b8rRysAqFCKfTcBtm_KJgNeZ-14L3e0 1m kubelet-bootstrap Pendingnode-csr-u1zKoBmUWwqCRMlQi_tbgbMX-63kZ7IrYNnAz0FgEUg 55s kubelet-bootstrap Pending# 手动 approve CSR 请求[root@k8s-master ~]# kubectl certificate approve node-csr-W9gLNYYT3Nk4b8rRysAqFCKfTcBtm_KJgNeZ-14L3e0[root@k8s-master ~]# kubectl certificate approve node-csr-u1zKoBmUWwqCRMlQi_tbgbMX-63kZ7IrYNnAz0FgEUg# 查看 CSR 列表：[root@k8s-master ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-W9gLNYYT3Nk4b8rRysAqFCKfTcBtm_KJgNeZ-14L3e0 2m kubelet-bootstrap Approved,Issuednode-csr-u1zKoBmUWwqCRMlQi_tbgbMX-63kZ7IrYNnAz0FgEUg 1m kubelet-bootstrap Approved,Issued部署 kube-proxy 组件kube-proxy 运行在所有 node节点上，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡创建 kube-proxy 配置文件123456[root@k8s-node1 ~]# vim /home/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=\"--logtostderr=true \\--v=4 \\--hostname-override=172.17.0.82 \\--cluster-cidr=10.0.0.0/24 \\--kubeconfig=/home/kubernetes/cfg/kube-proxy.kubeconfig\"参数说明：bindAddress: 监听地址；clientConnection.kubeconfig: 连接 apiserver 的 kubeconfig 文件；clusterCIDR: kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；hostnameOverride: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；mode: 使用 ipvs 模式；创建kube-proxy systemd unit 文件123456789101112[root@k8s-node1 ~]# vim /usr/lib/systemd/system/kube-proxy.service [Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=-/home/kubernetes/cfg/kube-proxyExecStart=/home/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTSRestart=on-failure[Install]WantedBy=multi-user.target启动kube-proxy服务1234[root@k8s-node1 ~]# systemctl daemon-reload[root@k8s-node1 ~]# systemctl enable kube-proxyCreated symlink from /etc/systemd/system/multi-user.target.wants/kube-proxy.service to /usr/lib/systemd/system/kube-proxy.service.[root@k8s-node1 ~]# systemctl restart kube-proxy查看集群状态给node节点配置label标签12[root@k8s-master ~]# kubectl label node 172.17.0.82 node-role.kubernetes.io/node='node1' [root@k8s-master ~]# kubectl label node 172.17.0.83 node-role.kubernetes.io/node='node2'查看集群及node节点状态1234567891011[root@k8s-master ~]# kubectl get node,csNAME STATUS ROLES AGE VERSIONnode/172.17.0.82 Ready node 29m v1.11.5node/172.17.0.83 Ready node 36m v1.11.5NAME STATUS MESSAGE ERRORcomponentstatus/scheduler Healthy ok componentstatus/controller-manager Healthy ok componentstatus/etcd-0 Healthy &#123;\"health\":\"true\"&#125; componentstatus/etcd-1 Healthy &#123;\"health\":\"true\"&#125; componentstatus/etcd-2 Healthy &#123;\"health\":\"true\"&#125;","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/categories/K8s/"},{"name":"Docker","slug":"K8s/Docker","permalink":"http://blog.wubolive.com/categories/K8s/Docker/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/tags/K8s/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.wubolive.com/tags/Docker/"}]},{"title":"Kubernetes 排错之 Pod 异常","slug":"k8s-pod-error","date":"2018-12-18T11:26:22.000Z","updated":"2018-12-18T11:15:27.720Z","comments":true,"path":"2018/12/18/k8s-pod-error/","link":"","permalink":"http://blog.wubolive.com/2018/12/18/k8s-pod-error/","excerpt":"","text":"本章介绍 Pod 运行异常的排错方法。一般来说，无论 Pod 处于什么异常状态，都可以执行以下命令来查看 Pod 的状态123kubectl get pod &lt;pod-name&gt; -o yaml 查看 Pod 的配置是否正确kubectl describe pod &lt;pod-name&gt; 查看 Pod 的事件kubectl logs &lt;pod-name&gt; [-c &lt;container-name&gt;] 查看容器日志这些事件和日志通常都会有助于排查 Pod 发生的问题。Pod 一直处于 Pending 状态Pending 说明 Pod 还没有调度到某个 Node 上面。可以通过 kubectl describe pod &lt;pod-name&gt; 命令查看到当前 Pod 的事件，进而判断为什么没有调度。可能的原因包括资源不足，集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 等资源HostPort 已被占用，通常推荐使用 Service 对外开放服务端口Pod 一直处于 Waiting 或 ContainerCreating 状态首先还是通过 kubectl describe pod命令查看到当前 Pod 的事件。可能的原因包括镜像拉取失败，比如配置了错误的镜像Kubelet 无法访问镜像（国内环境访问 gcr.io 需要特殊处理）私有镜像的密钥配置错误镜像太大，拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项）CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如无法配置 Pod 网络无法分配 IP 地址容器无法启动，需要检查是否打包了正确的镜像或者是否配置了正确的容器参数Pod 处于 ImagePullBackOff 状态这通常是镜像名称配置错误或者私有镜像的密钥配置错误导致。这种情况可以使用 docker pull &lt;image&gt; 来验证镜像是否可以正常拉取。如果是私有镜像，需要首先创建一个 docker-registry 类型的 Secret1kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL然后在容器中引用这个 Secret123456spec: containers: - name: private-reg-container image: &lt;your-private-image&gt; imagePullSecrets: - name: my-secretPod 一直处于 CrashLoopBackOff 状态CrashLoopBackOff 状态说明容器曾经启动了，但又异常退出了。此时可以先查看一下容器的日志12kubectl logs &lt;pod-name&gt;kubectl logs --previous &lt;pod-name&gt;这里可以发现一些容器退出的原因，比如容器进程退出健康检查失败退出此时如果还未发现线索，还可以到容器内执行命令来进一步查看退出原因1kubectl exec cassandra -- cat /var/log/cassandra/system.log如果还是没有线索，那就需要 SSH 登录该 Pod 所在的 Node 上，查看 Kubelet 或者 Docker 的日志进一步排查了12# 查询 Nodekubectl get pod &lt;pod-name&gt; -o widePod 处于 Error 状态通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括依赖的 ConfigMap、Secret 或者 PV 等不存在请求的资源超过了管理员设置的限制，比如超过了 - - LimitRange 等违反集群的安全策略，比如违反了 PodSecurityPolicy 等容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定Pod 处于 Terminating 或 Unknown 状态从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。想要删除这些状态的 Pod 有三种方法：从集群中删除该 Node。使用公有云时，kube-controller-manager 会在 VM 删除后自动删除对应的 Node。而在物理机部署的集群中，需要管理员手动删除 Node（如 kubectl delete node &lt;node-name&gt;。Node 恢复正常。Kubelet 会重新跟 kube-apiserver 通信确认这些 Pod 的期待状态，进而再决定删除或者继续运行这些 Pod。用户强制删除。用户可以执行 kubectl delete pods &lt;pod&gt; --grace-period=0 --force 强制删除 Pod。除非明确知道 Pod 的确处于停止状态（比如 Node 所在 VM 或物理机已经关机），否则不建议使用该方法。特别是 StatefulSet 管理的 Pod，强制删除容易导致脑裂或者数据丢失等问题。Pod 行为异常这里所说的行为异常是指 Pod 没有按预期的行为执行，比如没有运行 podSpec 里面设置的命令行参数。这一般是 podSpec yaml 文件内容有误，可以尝试使用 –validate 参数重建容器，比如12kubectl delete pod mypodkubectl create --validate -f mypod.yaml也可以查看创建后的 podSpec 是否是对的，比如1kubectl get pod mypod -o yaml修改静态 Pod 的 Manifest 后未自动重建Kubelet 使用 inotify 机制检测 /etc/kubernetes/manifests 目录（可通过 Kubelet 的 --pod-manifest-path 选项指定）中静态 Pod 的变化，并在文件发生变化后重新创建相应的 Pod。但有时也会发生修改静态 Pod 的 Manifest 后未自动创建新 Pod 的情景，此时一个简单的修复方法是重启 Kubelet。","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/categories/K8s/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/tags/K8s/"}]},{"title":"CentOS6 安装nfs共享目录服务","slug":"nfs-install","date":"2018-12-07T09:13:28.000Z","updated":"2018-12-07T09:15:50.636Z","comments":true,"path":"2018/12/07/nfs-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/07/nfs-install/","excerpt":"","text":"简介服务器IP：172.17.0.3客户端IP：172.17.0.10共享目录：/home/data服务端安装1[root@CentOS ~]# yum -y install nfs-utils rpcbind配置防火墙关闭selinux共享的目录：/home/data配置指定共享目录及主机12[root@CentOS ~]# vim /etc/exports/home/data 172.17.0.10(rw,no_root_squash,no_all_squash,sync)启动nfs及rpcbind1234[root@CentOS ~]# chkconfig nfs on[root@CentOS ~]# chkconfig rpcbind on[root@CentOS ~]# service nfs start[root@CentOS ~]# service rpcbind start客户端安装1[root@CentOS ~]# yum -y install nfs-utils查看服务端共享信息123[root@CentOS ~]# showmount -e 172.17.0.3Export list for 172.17.0.3:/home/data 172.17.0.10挂载nfs共享目录123[root@CentOS ~]# mkdir -p /home/data#挂载nfs,格式(mount -t nfs 服务器IP:/服务器目录 /本机挂载点)[root@CentOS ~]# mount -t nfs 172.17.0.3:/home/data /home/data配置开机自动挂载123[root@CentOS ~]# vim /etc/fstab在文件最后加入下行172.17.0.3:/home/data /home/data nfs defaults 0 0卸载共享目录1[root@CentOS ~]# umount 172.17.0.3:/home/data##nfs挂载出现nobody解决方法1mount -t nfs4 -o vers=3 ip:/pwd /pwd","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"NFS","slug":"NFS","permalink":"http://blog.wubolive.com/tags/NFS/"}]},{"title":"vncserver端口占用解决方法","slug":"vnc-pord","date":"2018-12-07T09:00:28.000Z","updated":"2018-12-07T09:18:42.748Z","comments":true,"path":"2018/12/07/vnc-pord/","link":"","permalink":"http://blog.wubolive.com/2018/12/07/vnc-pord/","excerpt":"","text":"在重启kvn物理服务器后启动vncserver服务时报错，如下123[root@node5 ~]# service vncserver startStarting VNC server: 1:root A VNC server is already running as :1 [FAILED]使用lsof命令查看5902端口，发现被kvn实例占用了12345678[root@node5 ~]# /usr/sbin/lsof -i tcp:5902COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEqemu-kvm 4214 qemu 15u IPv4 19697 0t0 TCP *:5902 (LISTEN)[root@node5 ~]# netstat -lntp | grep 5902 tcp 0 0 0.0.0.0:5902 0.0.0.0:* LISTEN 4214/qemu-kvm[root@node5 ~]# ps -ef |grep 4214qemu 4214 1 1 Sep23 ? 01:12:38 /usr/libexec/qemu-kvm -name node6-vm29.xxzx.local -S -M rhel6.6.0 -enable-kvm -m 8196 -realtime mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid ......省略数行......进过百度发现vncserver端口可以是5901,5902,5903…12345[root@node5 ~]# netstat -lntp | grep 590tcp 0 0 0.0.0.0:5900 0.0.0.0:* LISTEN 4124/qemu-kvm tcp 0 0 0.0.0.0:5901 0.0.0.0:* LISTEN 4175/qemu-kvm tcp 0 0 0.0.0.0:5902 0.0.0.0:* LISTEN 4214/qemu-kvm tcp 0 0 0.0.0.0:5903 0.0.0.0:* LISTEN 5799/qemu-kvm配置/etc/sysconfig/vncservers文件修改vncserver端口12345[root@node5 ~]# vim /etc/sysconfig/vncservers# 将下面数字配置成端口未被占用的尾数(比如说5904未被占用，则修改成4:root)VNCSERVERS=&quot;4:root&quot;VNCSERVERARGS[4]=&quot;-geometry 1280x800&quot;# 1280x800表示vnc客户端连接时的分辨率最后启动vncserver服务测试是否成功1234567891011121314[root@node5 ~]# service vncserver start Starting VNC server: 4:root New &apos;node5:4 (root)&apos; desktop is node5:4Starting applications specified in /root/.vnc/xstartupLog file is /root/.vnc/node5:4.log [ OK ][root@node5 ~]# vncserver -listTigerVNC server sessions:X DISPLAY # PROCESS ID:4 19336","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"Vnc","slug":"Vnc","permalink":"http://blog.wubolive.com/tags/Vnc/"}]},{"title":"rabbitMQ 安装","slug":"rebbitmq-install","date":"2018-12-07T07:00:28.000Z","updated":"2018-12-07T09:22:17.535Z","comments":true,"path":"2018/12/07/rebbitmq-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/07/rebbitmq-install/","excerpt":"","text":"安装依赖12yum install epel-releaseyum install erlang下载rpm包1wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.15/rabbitmq-server-3.6.15-1.el7.noarch.rpm安装rabbitmq-server1yum -y install rabbitmq-server-3.6.15-1.el7.noarch.rpm启动12systemctl start rabbitmq-serversystemctl enable rabbitmq-server查看当前所有用户1rabbitmqctl list_users查看默认guest用户的权限1rabbitmqctl list_user_permissions guest由于RabbitMQ默认的账号用户名和密码都是guest。为了安全起见, 先删掉默认用户1rabbitmqctl delete_user guest###添加新用户1rabbitmqctl add_user [admin] [admin123]设置用户tag1rabbitmqctl set_user_tags [admin] administrator赋予用户默认vhost的全部操作权限1rabbitmqctl set_permissions -p / [admin] &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;查看用户的权限1rabbitmqctl list_user_permissions [admin]开启web端口1rabbitmq-plugins enable rabbitmq_management","categories":[{"name":"MQ","slug":"MQ","permalink":"http://blog.wubolive.com/categories/MQ/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"rabbitMQ","slug":"rabbitMQ","permalink":"http://blog.wubolive.com/tags/rabbitMQ/"}]},{"title":"vncserver安装及配置","slug":"vnc-install","date":"2018-12-07T07:00:28.000Z","updated":"2018-12-07T09:23:08.607Z","comments":true,"path":"2018/12/07/vnc-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/07/vnc-install/","excerpt":"","text":"##说明安装服务的过程当中，最好是在联网环境下操作。由于涉及到远程连接的问题，系统的防火墙需要关闭，或者是做好相应的过滤策略。参考了网上很多大牛的意见和文章，结合自己的安装经历，现在总结一下。##1.下载并按照vnc-server###1.1 检查是否已经安装默认情况下，vnc服务是没有被系统安装的，可以通过以下命令查看：123[root@centos6 ~]# rpm -qa | grep vnctigervnc-server-1.1.0-24.el6.x86_64tigervnc-1.1.0-24.el6.x86_64执行查询命令，如果没有内容说明还没有安装vnc服务，需要安装。###1.2 下载和安装其实这个步骤很简单，都交给系统去做了，执行下面yum安装命令即可完成：1[root@centos6 ~]# yum install tigervnc tigervnc-server -y##2.配置vncserver打开配置文件1234[root@centos6 ~]# vim /etc/sysconfig/vncserver#在文件修改为下面两行代码：VNCSERVERS=&quot;1:root&quot;VNCSERVERARGS[1]=&quot;-geometry 1024x768 -nolisten tcp -localhost&quot;##3.设置或修改vnc登录密码配置当前用户的vnc登陆密码，连续输入两次即可。修改vnc的登录密码，也是和设置新密码一样的步骤。这个密码是vnc连接的独立密码，与系统密码没有关系。123[root@centos6 ~]# vncpasswd Password:Verify:##4.配置防火墙规则，允许vnc远程连接VNC server监听的端口从5900开始，display:1的监听 5901，display:2监听 5902，以此类推。CentOS 的防火墙缺省是不允许连接这些端口的，需要配置防火墙开放相关端口（root 权限)1234567[root@master /]# vi /etc/sysconfig/iptables...#开放 &quot;5902&quot; 端口...-A INPUT -m state --state NEW -m tcp -p tcp --dport 5901 -j ACCEPT #开放 &quot;5901&quot; 端口-A INPUT -m state --state NEW -m tcp -p tcp --dport 5902 -j ACCEPT #重启防火墙[root@master /]# service iptables restart##5.启动服务（开启多个远程端口）并设置开机自启动###5.1 启动方式一让系统自动分配连接端口：123456789[root@centos6 ~]# vncserver xauth: file /root/.Xauthority does not existxauth: (stdin):1: bad display name &quot;centos6.5-1:1&quot; in &quot;add&quot; commandNew &apos;centos6.5-1:1 (root)&apos; desktop is centos6.5-1:1Creating default startup script /root/.vnc/xstartupStarting applications specified in /root/.vnc/xstartupLog file is /root/.vnc/centos6.5-1:1.log直接输入则会自动打开一个连接服务“centos6.5-1:1”说明这是启动的第一个服务###5.2 启动方式二指定端口启动，如果指定端口的vnc服务已经启动则会提示，如果还没有启动则会重新启动特定端口的vnc服务：12[root@centos6 ~]# vncserver :1 #指定端口1（即系统的5901端口）启动服务，注意“:1”之前有一个空格[root@centos6 ~]# vncserver :2 #指定端口2（即系统的5902端口）启动服务###5.3 开机自启动设置开机自动启动服务：123[root@centos6 ~]# chkconfig vncserver on[root@centos6 ~]# chkconfig --list | grep vncservervncserver 0:off 1:off 2:on 3:on 4:on 5:on 6:off##6.windows环境远程连接服务器###6.1 客户端远程连接下载且安装好windows桌面的vnc-viewer客户端；然后在VNC Server（服务器）这一项输入“目标主机IP:登录端口（没错，端口就是上面的1或者2或3等）”；Encryption（加密）这一项采用默认方式（让vnc server自己选择）；点击“Connection（连接）”；输入设置的vnc登录密码（注意是vnc的密码，不是目标主机系统的登录密码）。###6.2 使用浏览器远程连接可以参考如下链接Chronme VNC远程连接插件##7.关闭vncserver执行kill命令：12345678910# 查看开启的vnc端口[root@master /]# vncserver -listTigerVNC server sessions:X DISPLAY # PROCESS ID:1 19558:2 20069:3 20447[root@master /]# vncserver -kill :1 #关闭端口1[root@master /]# vncserver -kill :2 #关闭端口2注意：-kill与:1或:2中间有一空格","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"Vnc","slug":"Vnc","permalink":"http://blog.wubolive.com/tags/Vnc/"}]},{"title":"Harbr更改端口","slug":"k8s-harbor-port","date":"2018-12-04T02:36:28.000Z","updated":"2018-12-04T01:12:07.205Z","comments":true,"path":"2018/12/04/k8s-harbor-port/","link":"","permalink":"http://blog.wubolive.com/2018/12/04/k8s-harbor-port/","excerpt":"","text":"前言默认情况下，Harbor的侦听端口80(HTTP)和443(HTTPS)，因某原因无法使用80或443端口可按照本方法更改。修改docker-compose.yml文件找到nginx镜像配置,将外网映射端口改成未被占用的端口123456789101112131415161718[root@hub harbor]# vim docker-compose.yml proxy: image: goharbor/nginx-photon:v1.6.2 container_name: nginx restart: always volumes: - ./common/config/nginx:/etc/nginx:z networks: - harbor ports: - 88:80 - 843:443 - 8443:4443 depends_on: - postgresql - registry - ui - log更改harbor.cfg文件将hostname添加上映射端口12[root@hub harbor]# vim harbor.cfg hostname = hub.wubolive.com:88重启docker-compose12[root@hub harbor]# docker-compose down -v[root@hub harbor]# ./install","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/categories/K8s/"},{"name":"Docker","slug":"K8s/Docker","permalink":"http://blog.wubolive.com/categories/K8s/Docker/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/tags/K8s/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.wubolive.com/tags/Docker/"}]},{"title":"Harbor私有仓库安装","slug":"k8s-harbor-install","date":"2018-12-04T01:11:28.000Z","updated":"2018-12-04T01:11:55.201Z","comments":true,"path":"2018/12/04/k8s-harbor-install/","link":"","permalink":"http://blog.wubolive.com/2018/12/04/k8s-harbor-install/","excerpt":"","text":"Harbor安装https://github.com/vmware/harbor/releases 中下载Harbor online installer离线安装包123[root@hub ~]# wget https://storage.googleapis.com/harbor-releases/release-1.6.0/harbor-online-installer-v1.6.2.tgz[root@hub ~]# tar zxf harbor-online-installer-v1.6.2.tgz [root@hub ~]# cd harbor/配置harbor如果不想已80端口运行，请查看：http://www.wubolive.com/post/harbor-port123456789#更改映射端口可修改[root@hub harbor]# vim harbor.cfghostname = hub.wubolive.com #配置访问地址ui_url_protocol = http harbor_admin_password = 123456 #Web登录页面密码# 准备配置文件[root@hub harbor]# ./prepare# 安装启动harbor[root@hub harbor]# ./install.sh查看以运行的docker容器12345678910[root@hub harbor]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc39813d51868 goharbor/nginx-photon:v1.6.2 &quot;nginx -g &apos;daemon of…&quot; 3 minutes ago Up 3 minutes (healthy) 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp nginxc80e7a781c7a goharbor/harbor-jobservice:v1.6.2 &quot;/harbor/start.sh&quot; 3 minutes ago Up 3 minutes harbor-jobservice189ed8429b30 goharbor/harbor-ui:v1.6.2 &quot;/harbor/start.sh&quot; 3 minutes ago Up 3 minutes (healthy) harbor-ui99774d0dd424 goharbor/redis-photon:v1.6.2 &quot;docker-entrypoint.s…&quot; 3 minutes ago Up 3 minutes 6379/tcp redis9eead178687e goharbor/harbor-db:v1.6.2 &quot;/entrypoint.sh post…&quot; 3 minutes ago Up 3 minutes (healthy) 5432/tcp harbor-db82a2cfce37ee goharbor/harbor-adminserver:v1.6.2 &quot;/harbor/start.sh&quot; 3 minutes ago Up 3 minutes (healthy) harbor-adminserver9314f9963d75 goharbor/registry-photon:v2.6.2-v1.6.2 &quot;/entrypoint.sh /etc…&quot; 3 minutes ago Up 3 minutes (healthy) 5000/tcp registry3cdab5ca0e61 goharbor/harbor-log:v1.6.2 &quot;/bin/sh -c /usr/loc…&quot; 3 minutes ago Up 3 minutes (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-log访问Harbor浏览器输入：http://hub.wubolive.com登入账号：admin登入密码：123456在面板中创建一个用户K8S并加入library项目上传docker镜像因docker默认不支持http方式上传镜像，所有需要在配置文件中指明使用http访问12345[root@hub harbor]# vim /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;http://cc51a693.m.daocloud.io&quot;], &quot;insecure-registries&quot;: [&quot;hub.wubolive.com&quot;]&#125;登入Harbor12345[root@hub harbor]# docker login hub.wubolive.comUsername: k8sPassword: Login Succeeded# 显示Login Succeeded说明登入成功推送镜像格式：1234# docker tag 镜像中心域名/项目名称/镜像名:版本docker tag SOURCE_IMAGE[:TAG] hub.wubolive.com/library/IMAGE[:TAG]# docker push 更改tag后的镜像名docker push hub.wubolive.com/library/IMAGE[:TAG]拉取镜像格式：1docker pull hub.wubolive.com/library/nginx:latest附：以HTTPS方式部署Harborhttps://github.com/goharbor/harbor/blob/master/docs/configure_https.md","categories":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/categories/K8s/"},{"name":"Docker","slug":"K8s/Docker","permalink":"http://blog.wubolive.com/categories/K8s/Docker/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://blog.wubolive.com/tags/K8s/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.wubolive.com/tags/Docker/"}]},{"title":"Kvm虚拟机迁移方案","slug":"kvm-scp","date":"2018-12-01T02:00:28.000Z","updated":"2018-12-07T09:25:22.014Z","comments":true,"path":"2018/12/01/kvm-scp/","link":"","permalink":"http://blog.wubolive.com/2018/12/01/kvm-scp/","excerpt":"","text":"在node1上的img镜像文件拷贝到node11上1[root@node1 ~]# scp /home/mv/vm01.centos.local.img root@172.17.0.12:/home/vm在node1上执行迁移命令（源主机和目标主机存放镜像的路径必须保持一致，否则报错）123[root@node1 ~]# virsh migrate --live --verbose vm01.centos.local qemu+ssh://172.17.0.12/system tcp://172.17.0.12root@172.17.0.12&apos;s password: Migration: [100 %]###在迁移过程中在本机一直ping正在迁移的虚拟机1C:\\Users\\Administrator&gt;ping -t 172.17.0.55中间会有短暂的延迟，不过妨碍不大到node11上生成虚拟机配置文件1[root@node11 ~]# virsh dumpxml vm01.centos.local &gt; /etc/libvirt/qemu/vm01.centos.local.xml###通过迁移后生成的配置文件从新定义虚拟机` [root@node11 ~]# virsh define /etc/libvirt/qemu/vm01.centos.local.xml Domain vm01.centos.local defined from /etc/libvirt/qemu/vm01.centos.local.xml","categories":[{"name":"Kvm","slug":"Kvm","permalink":"http://blog.wubolive.com/categories/Kvm/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"},{"name":"Kvm","slug":"Kvm","permalink":"http://blog.wubolive.com/tags/Kvm/"}]},{"title":"Linux以虚拟用户方式安装Vsftpd","slug":"vsftpd-install","date":"2018-11-30T06:36:28.000Z","updated":"2019-05-13T09:52:02.196Z","comments":true,"path":"2018/11/30/vsftpd-install/","link":"","permalink":"http://blog.wubolive.com/2018/11/30/vsftpd-install/","excerpt":"","text":"说明：vsftpd的版本：vsftpd-3.0.2-22.el7.x86_64ftp 根目录 : /data/ftpftp 配置文件目录：/etc/vsftpdftp 虚拟用户权限配置文件目录：/etc/vsftpd/vuser_config实现目标：匿名用户可以登录，但是不能访问虚拟用户的宿主目录，只能访问共享目录虚拟用户对自己的宿主目录有任何权限，且只能在自己宿主目录中操作搭建过程1.安装vsftpd，ftp和libdb-utils(需要安装db包，用来加密虚拟用户的账户信息,centos7默认安装好了)1[root@CentOS ~]# yum install vsftpd ftp -y2.创建本地用户[用于映射虚拟用户]1234567#建立ftp用户目录[root@CentOS ~]# mkdir -p /ftp-dir#创建用户[root@CentOS ~]# useradd -d /ftp-dir/ vftpuser -s /sbin/nologin #更改权限和主组权限[root@CentOS ~]# chmod 755 /ftp-dir[root@CentOS ~]# chown vftpuser.root /ftp-dir3.创建虚拟用户[用户和密码]文件12345[root@CentOS ~]# vim /etc/vsftpd/vuseradmin [用户名]123456 [密码]devops [用户名]123456 [密码]4.加密用户密码文件生成数据库文件123[root@CentOS ~]# cd /etc/vsftpd/[root@CentOS ~]# db_load -T -t hash -f ./vuser ./login.db[root@CentOS ~]# chmod 600 login.db5.创建PAM认证文件1234[root@CentOS ~]# vim /etc/pam.d/vsftpd.vuauth required /lib64/security/pam_userdb.so db=/etc/vsftpd/login #注意64位系统写/lib64这个路径，32位系统要写成/lib，下同！account required /lib64/security/pam_userdb.so db=/etc/vsftpd/login6.修改配置文件1234567891011121314151617181920212223242526272829303132[root@CentOS ~]# vi /etc/vsftpd/vsftpd.conf#允许匿名用户访问anonymous_enable=yeslocal_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_file=/var/log/xferlogxferlog_std_format=YESlisten=NOlisten_ipv6=YES#修改ftp默认目录到/ftp-dir下面chroot_local_user=YESlocal_root=/ftp-diranon_root=/ftp-dir#pam认证文件pam_service_name=vsftpd.vu#虚拟用户权限配置目录user_config_dir=/etc/vsftpd/ftploginuserlist_enable=YEStcp_wrappers=YESallow_writeable_chroot=YESone_process_model=NO#开启虚拟用户guest_enable=YESguest_username=vftpuser注意，如果vsftpd的版本是：vsftpd-2.2.2-24.el6.x86_64那么vsftpd.conf的配置文件修改如下,其他操作一样，不然的话2.2.2版本启动时会遇到各种问题：123456789101112131415161718listen=YESlocal_enable=YESanon_umask=022anonymous_enable=YESpam_service_name=vsftpd.vuuserlist_enable=YESchroot_local_user=YESlocal_root=/ftp-diranon_root=/ftp-dirguest_enable=YESguest_username=vftpuseruser_config_dir=/etc/vsftpd/ftploginuserlist_enable=YESxferlog_enable=YESxferlog_std_format=YESxferlog_file=/var/log/xferlogdual_log_enable=YESvsftpd_log_file=/var/log/vsftpd.log7.重启vsftpd服务1[root@CentOS ~]# systemctl restart vsftpd8.创建虚拟用户[权限]配置文件123456789101112131415[root@CentOS ~]# mkdir /etc/vsftpd/ftplogin[root@CentOS ~]# cd /etc/vsftpd/ftplogin[root@CentOS ~]# vi abc [有所有权限]#设置登录后禁锢的目录local_root=/ftp-dir/admin#开放写权限write_enable=yes#开放下载权限anon_world_readable_only=no#开放上传权限anon_upload_enable=yes#开放创建目录的权限anon_mkdir_write_enable=yes#开放删除和重命名的权限anon_other_write_enable=yes1234# vi bcd [只有上传下载的权限]local_root=/ftp-dir/devopsanon_upload_enable=yesanon_world_readable_only=no9.更改虚拟用户目录权限1234#如果不更改的话，匿名用户是可以访问到的[root@CentOS ~]# mkdir /ftp-dir/admin &amp;&amp; chmod 700 /ftp-dir/admin[root@CentOS ~]# mkdir /ftp-dir/devops &amp;&amp; chmod 700 /ftp-dir/devops[root@CentOS ~]# chown -R vftpuser.root /ftp-dir10.测试访问。1234567891011[root@CentOS ~]# ftp 127.0.0.1Connected to 127.0.0.1 (127.0.0.1).220 (vsFTPd 3.0.2)Name (127.0.0.1:root): devops331 Please specify the password.Password:230 Login successful.Remote system type is UNIX.Using binary mode to transfer files.ftp&gt; exit221 Goodbye.11.添加用户(不需要重启vsftpd服务)1234567891011121314#添加新用户test，密码为abcd[root@CentOS ~]# vim /etc/vsftpd/vuseradmin123456devops123456user1123456#创建test用户对应的目录并授权[root@CentOS ~]# mkdir /ftp-dir/user1 &amp;&amp; chown vuser:root /ftp-dir/user1#重新生成加密的db文件[root@CentOS ~]# cd /etc/vsftpd[root@CentOS ~]# db_load -T -t hash -f ./vuser ./login.db#删除用户就是把添加用户的操作撤销，然后删除加密的db文件重新生成即可。12.测试访问，此时添加的test用户的/ftp-dir/test目录的权限是755，匿名用户可以登录。若要屏蔽，修改权限为700即可。1[root@CentOS ~]# ftp 127.0.0.113.错误处理错误1：226 Transfer done (but failed to open directory)解决：selinux 和防火墙导致错误2：500 OOPS: vsftpd: refusing to run with writable root inside chroot()解决：配置文件中加入 allow_writeable_chroot=YES 针对标准vsftpd(standonly)模式，然后重启ftp.vsftp上传文件权限问题file_open_mode上传档案的权限，与chmod 所使用的数值相同。如果希望上传的文件可以执行，设此值为0777。默认情况下vsftp上传之后文件的权限是600，目录权限是700local_umask=xxx这是指定本地用户上传后的文件权限设置anon_umask=xxx这是指定虚拟用户上传后的文件权限设置umask是unix操作系统的概念，umask决定目录和文件被创建时得到的初始权限umask = 022时，新建的目录 权限是755，文件的权限是 644umask = 077时，新建的目录 权限是700，文件的权限时 600","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/categories/Linux/"}],"tags":[{"name":"FTP","slug":"FTP","permalink":"http://blog.wubolive.com/tags/FTP/"}]},{"title":"Ansible Ad-Hoc命令集","slug":"ansible-hoc","date":"2018-11-30T06:13:28.000Z","updated":"2019-05-14T01:45:05.137Z","comments":true,"path":"2018/11/30/ansible-hoc/","link":"","permalink":"http://blog.wubolive.com/2018/11/30/ansible-hoc/","excerpt":"","text":"ansible命令使用方法123456789101112131415161718192021222324ansible &lt;host-pattern&gt; [options]可用选项如下。 -v,--verbose:输出更详细的执行过程信息,-vvv可得到执行过程所有信息。 -i PATH,--inventory=PATH:指定inventory信息,默认/etc/absible/hosts。 -f NUM,--forks=NUM:并发线程数,默认5个线程。 --private-key=PRIVATE_KEY_FILE:指定密钥文件。 -m NAME,--module-name=NAME:指定执行使用的模块。 -M DIRECTORY,--module-path=DIRECTORY:指定模块存放路径,默认/usr/share/ansible,也可以通过ANSIBLE_LIBRARY设定默认路径。 -a &apos;ARGUMENTS&apos;,--args=&apos;ARGUMENTS&apos;:模块参数。 -k,--ask-pass SSH:认证密码。 -K,--ask-sudo-pass sudo:用户的密码(--sudo时使用)。 -o,--one-line:标准输出至一行。 -s,--sudo:相当于Linux系统下的sudo命令。 -t DIRECTORY,--tree=DIRECTORY:输出信息至DIRECTORY目录下,结果文件以远程主机名命名。 -T SECONDS,--timeout=SECONDS:指定连接远程主机的最大超时,单位是秒。 -B NUM,--background=NUM:后台执行命令,超NUM秒后中止正在执行的任务。 -P NUM,--poll=NUM:定期返回后台任务进度。 -u USERNAME,--user=USERNAME:指定远程主机以USERNAME运行命令。 -U SUDO_USERNAME,--sudo-user=SUDO_USERNAME:使用sudo,相当于Linux下的sudo命令。 -c CONNECTION,--connection=CONNECTION:指定连接方式,可用选项paramiko(SSH)、ssh、local,local方式常 用于crontab和kickstarts。 -l SUBSET,--limit=SUBSET:指定运行主机。 -l ~REGEX,--limit=~REGEX:指定运行主机(正则)。 --list-hosts:列出符合条件的主机列表,不执行任何命令。情景1:检查all组所有主机是否存活12345678910111213[root@Ansible ~]# ansible all -f 5 -m ping36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;情景2:返回all组所有主机的hostname123456789[root@Ansible ~]# ansible all -s -m command -a &apos;hostname&apos; 36.103.245.138 | SUCCESS | rc=0 &gt;&gt;node136.103.245.70 | SUCCESS | rc=0 &gt;&gt;node236.103.245.135 | SUCCESS | rc=0 &gt;&gt;master[流程图](leanote://file/getImage?fileId=5b8f7cc3ab6441765b001637)情景3：列出all组所有主机列表12345[root@Ansible ~]# ansible all --list hosts (3): 36.103.245.135 36.103.245.138 36.103.245.70情景4：列出all组所有主机磁盘使用情况1234567891011121314151617181920212223[root@Ansible ~]# ansible all -a &apos;df -lh&apos;36.103.245.138 | SUCCESS | rc=0 &gt;&gt;Filesystem Size Used Avail Use% Mounted ondevtmpfs 7.9G 0 7.9G 0% /devtmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 7.9G 57M 7.8G 1% /run/dev/vda2 36G 2.8G 31G 9% /36.103.245.70 | SUCCESS | rc=0 &gt;&gt;Filesystem Size Used Avail Use% Mounted ondevtmpfs 7.9G 0 7.9G 0% /devtmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 7.9G 57M 7.8G 1% /run/dev/vda2 36G 2.8G 31G 9% //dev/vdb1 99G 61M 94G 1% /home36.103.245.135 | SUCCESS | rc=0 &gt;&gt;Filesystem Size Used Avail Use% Mounted ondevtmpfs 7.9G 0 7.9G 0% /devtmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 7.9G 185M 7.7G 3% /run/dev/vda2 36G 25G 9.4G 72% //dev/vdb1 99G 61M 94G 1% /home情景5：列出all组所有主机内存使用情况123456789101112131415[root@Ansible ~]# ansible all -m shell -a &quot;free -h&quot;36.103.245.138 | SUCCESS | rc=0 &gt;&gt; total used free shared buff/cache availableMem: 15G 384M 13G 56M 1.4G 14GSwap: 0B 0B 0B36.103.245.70 | SUCCESS | rc=0 &gt;&gt; total used free shared buff/cache availableMem: 15G 377M 13G 56M 1.3G 14GSwap: 0B 0B 0B36.103.245.135 | SUCCESS | rc=0 &gt;&gt; total used free shared buff/cache availableMem: 15G 839M 11G 184M 3.1G 14GSwap: 0B 0B 0Bansible-doc命令使用方法123456789ansible-doc [options] [module...]可用选项如下。 --version:显示工具版本号 -h,--help:显示该help说明 -M MODULE_PATH,--module-path=MODULE_PATH:指定Ansible模块的默认加载目录。 -l,--list:列出所有可用的模块。 -s,--snippet:只显示playbook说明的代码段。 -v:显示工具版本号。【示例1】安装redhat-lsb并查看服务器系统版本号。步骤1：安装redhat-lsb123456789101112131415161718192021222324[root@Ansible ~]# ansible all -m yum -a &quot;name=redhat-lsb state=present&quot;36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;redhat-lsb-4.1-27.el7.centos.1.x86_64 providing redhat-lsb is already installed&quot; ]&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;redhat-lsb-4.1-27.el7.centos.1.x86_64 providing redhat-lsb is already installed&quot; ]&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;redhat-lsb-4.1-27.el7.centos.1.x86_64 providing redhat-lsb is already installed&quot; ]步骤2：查看系统版本号123456789101112131415161718192021[root@Ansible ~]# ansible all -m command -a &quot;lsb_release -a&quot;36.103.245.138 | SUCCESS | rc=0 &gt;&gt;LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611Codename: Core36.103.245.70 | SUCCESS | rc=0 &gt;&gt;LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611Codename: Core36.103.245.135 | SUCCESS | rc=0 &gt;&gt;LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611Codename: Core【示例2】为所有服务器安装ntp服务,并设置为开机启动步骤1：安装ntp服务12345678910111213141516171819202122232425[root@Ansible ~]# ansible all -s -m yum -a &quot;name=ntp state=present&quot;36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;ntp-4.2.6p5-28.el7.centos.x86_64 providing ntp is already installed&quot; ]&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;ntp-4.2.6p5-28.el7.centos.x86_64 providing ntp is already installed&quot; ]&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;&quot;, &quot;rc&quot;: 0, &quot;results&quot;: [ &quot;ntp-4.2.6p5-28.el7.centos.x86_64 providing ntp is already installed&quot; ]&#125;步骤2：启动ntp服务,并设置为开机启动12345678910111213141516171819202122232425262728[root@Ansible ~]# ansible all -m service -a &quot;name=ntpd state=started enabled=yes&quot;36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;enabled&quot;: true, &quot;name&quot;: &quot;ntpd&quot;, &quot;state&quot;: &quot;started&quot;, &quot;status&quot;: &#123; ...... &#125;&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;enabled&quot;: true, &quot;name&quot;: &quot;ntpd&quot;, &quot;state&quot;: &quot;started&quot;, &quot;status&quot;: &#123; ...... &#125;&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;enabled&quot;: true, &quot;name&quot;: &quot;ntpd&quot;, &quot;state&quot;: &quot;started&quot;, &quot;status&quot;: &#123; ...... &#125;&#125;","categories":[{"name":"Ansable","slug":"Ansable","permalink":"http://blog.wubolive.com/categories/Ansable/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://blog.wubolive.com/tags/Ansible/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"}]},{"title":"Ansible常用模块收录册","slug":"ansible-mode","date":"2018-11-30T06:11:28.000Z","updated":"2018-11-30T13:43:17.726Z","comments":true,"path":"2018/11/30/ansible-mode/","link":"","permalink":"http://blog.wubolive.com/2018/11/30/ansible-mode/","excerpt":"","text":"Ansible常用模块，长期更新…yum模块yum：RedHat/CentOS包管理工具12345678910111213常用选项：– config_file：yum的配置文件 （optional） – disable_gpg_check：关闭gpg_check （optional） – disablerepo：不启用某个源 （optional） – enablerepo：启用某个源（optional） – name：要进行操作的软件包的名字，默认最新的程序包，指明要安装的程序包，可以带上版本号，也可以传递一个url或者一个本地的rpm包的路径 – state：状态（present，absent，latest），表示是安装还卸载 present:默认的，表示为安装 lastest: 安装为最新的版本 absent：表示删除 示例：[root@Ansible ~]# ansible all -m yum -a &apos;name=httpd state=latest&apos;","categories":[{"name":"Ansable","slug":"Ansable","permalink":"http://blog.wubolive.com/categories/Ansable/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://blog.wubolive.com/tags/Ansible/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"}]},{"title":"Ansible安装与配置","slug":"ansible-install","date":"2018-11-30T06:10:28.000Z","updated":"2018-11-30T13:43:11.878Z","comments":true,"path":"2018/11/30/ansible-install/","link":"","permalink":"http://blog.wubolive.com/2018/11/30/ansible-install/","excerpt":"","text":"本文讲述如何安装Ansible环境，了解Ansible基本配置、运行测试安装AnsibleCentOS 7系统安装Ansible(yum方式)1234#安装ansible yum源rpm -Uvh http://mirrors.zju.edu.cn/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm#yum安装ansibleyum -y install ansible配置Ansible主配置文件常用配置12345678910111213141516171819[root@CentOS ~]# vim /etc/ansible/ansible.cfg[defaults]# 存放主机列表文件inventory = /etc/ansible/hosts# 指向Ansible模块目录library = /usr/share/my_modules/# 配置Ansible最大运行进程forks = 5# 设置默认执行命令的用户sudo_user = root# 配置管理节点的管理端口remote_port = 22# 配置是否检查SSH主机密钥host_key_checking = False# SSH超时时间timeout = 60# 指定存放Ansible日志的文件log_path = /var/log/ansible.log业务环境角色主机名IP地址组名控制主机ansable36.103.245.156—被管理节点master36.103.245.135k8server被管理节点node136.103.245.138k8server被管理节点node236.103.245.70k8server配置Linux主机ssh无密钥访问12345678910111213141516171819202122232425#生成密钥[root@Ansible ~]# ssh-keygen#将公钥分发到被管理节点[root@Ansible ~]# ssh-copy-id -i .ssh/id_rsa.pub root@36.103.245.135/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;.ssh/id_rsa.pub&quot;The authenticity of host &apos;36.103.245.135 (36.103.245.135)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:ZTtQLCTg21cYLQ5iJa5LkC51xN6lKGxVyLRAxjXPUOw.ECDSA key fingerprint is MD5:6d:5b:e9:d9:bd:12:64:06:c5:cc:a2:07:a6:99:96:3d.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@36.103.245.135&apos;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &apos;root@36.103.245.135&apos;&quot;and check to make sure that only the key(s) you wanted were added.## 对node1及node2作相同的配置#测试是否免密登入[root@Ansible ~]# ssh 36.103.245.135Last failed login: Wed Sep 5 10:06:49 CST 2018 from 118.24.129.24 on ssh:nottyThere were 27 failed login attempts since the last successful login.Last login: Wed Sep 5 09:50:20 2018 from 36.103.245.156Ansible 小测试首先可以查看一下ansible的软件版本信息1234567[root@Ansible ~]# ansible --versionansible 2.6.3 config file = /etc/ansible/ansible.cfg configured module search path = [u&apos;/usr/share/my_modules&apos;] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Nov 6 2016, 00:28:07) [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)]主机连通性测试使用Ansible需要先定于主机与组的配置，默认文件在/etc/ansible/hosts12345678910111213141516[root@Ansible ~]# cat /etc/ansible/hosts# 定义主机#master.wubolive.com#node1.wubolive.com#node2.wubolive.com36.103.245.13536.103.245.13836.103.245.70# 定义组(组名需要用[]括起来)[k8server]#master.wubolive.com#node1.wubolive.com#node2.wubolive.com36.103.245.13536.103.245.13836.103.245.70定义完成后使用ping模块对单主机进行ping操作12345[root@Ansible ~]# ansible 36.103.245.135 -m ping36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;对k8server组进行ping操作12345678910111213[root@Ansible ~]# ansible k8server -m ping 36.103.245.138 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;36.103.245.70 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;36.103.245.135 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;在被管理节点批量执行命令在用户目录创建一个资源清单文件inventory.cfg12345[root@Ansible ~]# cat inventory.cfg [k8server]36.103.245.13536.103.245.13836.103.245.70用ansible的shell模块对k8server组各服务器显示‘hello ansible！’123456789[root@Ansible ~]# ansible k8server -m shell -a &apos;/bin/echo hello ansible!&apos; -i inventory.cfg 36.103.245.70 | SUCCESS | rc=0 &gt;&gt;hello ansible!36.103.245.138 | SUCCESS | rc=0 &gt;&gt;hello ansible!36.103.245.135 | SUCCESS | rc=0 &gt;&gt;hello ansible!用command模块也可以执行相同操作123456789[root@Ansible ~]# ansible k8server -m command -a &apos;/bin/echo hello ansible!&apos; -i inventory.cfg 36.103.245.138 | SUCCESS | rc=0 &gt;&gt;hello ansible!36.103.245.70 | SUCCESS | rc=0 &gt;&gt;hello ansible!36.103.245.135 | SUCCESS | rc=0 &gt;&gt;hello ansible!获取帮助信息12345678910# 查看ansible命令帮助使用-h选项[root@Ansible ~]# ansible -h# 用ansible-doc列出ansible系统支持的模块[root@Ansible ~]# ansible-doc -l# 用ansible-doc加模块名称，可以显示该模块的描述和使用示例[root@Ansible ~]# ansible-doc yum# ansible-doc -s选项可以列出模块的动作[root@Ansible ~]# ansible-doc -s yum另外，在Ansible调试脚本时，可以使用-v或者-vvv显示详细的输出结果123[root@Ansible ~]# ansible k8server -i inventory.cfg -m ping -v# 或[root@Ansible ~]# ansible k8server -i inventory.cfg -m ping -vvv","categories":[{"name":"Ansable","slug":"Ansable","permalink":"http://blog.wubolive.com/categories/Ansable/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://blog.wubolive.com/tags/Ansible/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.wubolive.com/tags/Linux/"}]}]}